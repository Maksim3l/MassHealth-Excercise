{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2417ab30",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73183f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re\n",
    "import uuid\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.callbacks import *\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "try:\n",
    "\n",
    "    import tensorflow_directml as tf_directml\n",
    "    print(\"DirectML plugin loaded successfully!\")\n",
    "    tf_directml.enable_mixed_precision()\n",
    "    print(\"Available devices:\")\n",
    "    for device in tf.config.list_physical_devices():\n",
    "        print(f\"  {device}\")\n",
    "\n",
    "    physical_devices = tf.config.list_physical_devices('DML')\n",
    "    if physical_devices:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(f\"DirectML devices configured: {len(physical_devices)} devices\")\n",
    "    else:\n",
    "        print(\"No DirectML devices found, falling back to CPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"tensorflow-directml not found. Install with: pip install tensorflow-directml\")\n",
    "    print(\"Falling back to CPU training\")\n",
    "\n",
    "try:\n",
    "    set_global_policy('mixed_float16')\n",
    "except:\n",
    "    print(\"Mixed precision not supported, using float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = os.path.join('data', 'training')\n",
    "TEST = os.path.join('data', 'test')\n",
    "ARCH = os.path.join('data', 'archive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectMLCompatibleEarlyStopping(Callback):\n",
    "    \"\"\"DirectML-optimized early stopping callback\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data=None, patience=5, min_delta=0.01, \n",
    "                 min_verification_acc=0.90, restore_best_weights=True, verbose=1):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_verification_acc = min_verification_acc\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        self.best_verification_acc = 0\n",
    "        self.best_weights = None\n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            val_acc = self.calculate_verification_accuracy()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch + 1} - Verification Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > self.best_verification_acc + self.min_delta:\n",
    "                self.best_verification_acc = val_acc\n",
    "                self.wait = 0\n",
    "                if self.restore_best_weights:\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if val_acc < self.min_verification_acc:\n",
    "                print(f\"Verification accuracy dropped below {self.min_verification_acc}. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Verification accuracy hasn't improved for {self.patience} epochs. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            if self.model.stop_training and self.restore_best_weights and self.best_weights:\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                print(f\"Restored best weights with verification accuracy: {self.best_verification_acc:.4f}\")\n",
    "    \n",
    "    def calculate_verification_accuracy(self):\n",
    "        \"\"\"DirectML-optimized verification accuracy calculation\"\"\"\n",
    "        anchor_imgs, comparison_imgs, labels = self.validation_data\n",
    "        \n",
    "        # Smaller batch size for DirectML stability\n",
    "        batch_size = 16  # Reduced from 32\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(anchor_imgs), batch_size):\n",
    "            try:\n",
    "                batch_anchors = anchor_imgs[i:i+batch_size]\n",
    "                batch_comparisons = comparison_imgs[i:i+batch_size]\n",
    "                \n",
    "                processed_anchors = []\n",
    "                processed_comparisons = []\n",
    "                \n",
    "                for img in batch_anchors:\n",
    "                    if isinstance(img, str):\n",
    "                        processed_img = preprocess_with_augmentation(img, is_training=False)\n",
    "                    else:\n",
    "                        processed_img = tf.cast(img, tf.float32) / 255.0 if tf.reduce_max(img) > 1.0 else img\n",
    "                    processed_anchors.append(processed_img)\n",
    "                \n",
    "                for img in batch_comparisons:\n",
    "                    if isinstance(img, str):\n",
    "                        processed_img = preprocess_with_augmentation(img, is_training=False)\n",
    "                    else:\n",
    "                        processed_img = tf.cast(img, tf.float32) / 255.0 if tf.reduce_max(img) > 1.0 else img\n",
    "                    processed_comparisons.append(processed_img)\n",
    "                \n",
    "                processed_anchors = tf.stack(processed_anchors)\n",
    "                processed_comparisons = tf.stack(processed_comparisons)\n",
    "                \n",
    "                # DirectML-compatible prediction\n",
    "                with tf.device('/DML:0' if tf.config.list_physical_devices('DML') else '/CPU:0'):\n",
    "                    model_output = self.model([processed_anchors, processed_comparisons])\n",
    "                    if isinstance(model_output, list):\n",
    "                        batch_preds = model_output[-1]\n",
    "                    else:\n",
    "                        batch_preds = model_output\n",
    "                        \n",
    "                predictions.extend(batch_preds.numpy().flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        binary_preds = (np.array(predictions) > 0.5).astype(int)\n",
    "        return accuracy_score(labels[:len(binary_preds)], binary_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectMLGPUMonitor(Callback):\n",
    "    \"\"\"DirectML-specific GPU utilization monitor\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # More aggressive garbage collection for DirectML\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Force garbage collection on DirectML device\n",
    "        if tf.config.list_physical_devices('DML'):\n",
    "            try:\n",
    "                # DirectML-specific memory cleanup\n",
    "                tf.config.experimental.reset_memory_stats('DML:0')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750a529",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directml_data_pipeline(anchors, comparisons, labels, batch_size=32, \n",
    "                                 prefetch_buffer=tf.data.AUTOTUNE, num_parallel_calls=4):\n",
    "    \"\"\"Create DirectML-optimized data pipeline\"\"\"\n",
    "    \n",
    "    # Reduce parallelism for DirectML stability\n",
    "    if num_parallel_calls == tf.data.AUTOTUNE:\n",
    "        num_parallel_calls = 4  # Fixed value for DirectML\n",
    "    \n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(comparisons)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "    \n",
    "    # Smaller shuffle buffer for DirectML\n",
    "    dataset = dataset.shuffle(buffer_size=min(5000, len(anchors)))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda a, c, l: (\n",
    "            (preprocess_with_augmentation(a, is_training=True),\n",
    "             preprocess_with_augmentation(c, is_training=True)),\n",
    "            tf.cast(l, tf.float32)\n",
    "        ),\n",
    "        num_parallel_calls=num_parallel_calls\n",
    "    )\n",
    "\n",
    "    # Smaller batch size for DirectML\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(2)  # Reduced prefetch for DirectML\n",
    "    \n",
    "    # More conservative caching for DirectML\n",
    "    if len(anchors) < 20000:  # Reduced threshold\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directml_augmentation():\n",
    "    \"\"\"DirectML-compatible augmentation pipeline\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.05),  # Reduced rotation\n",
    "        tf.keras.layers.RandomZoom(0.05),      # Reduced zoom\n",
    "        tf.keras.layers.RandomContrast(0.05),  # Reduced contrast\n",
    "    ])\n",
    "\n",
    "DIRECTML_AUGMENTATION = create_directml_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_augmentation(img_path, is_training=True):\n",
    "    \"\"\"DirectML-optimized preprocessing\"\"\"\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    \n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if is_training:\n",
    "        img = DIRECTML_AUGMENTATION(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_pairs_from_directory(directory, max_people=1000, max_pairs_per_person=100):\n",
    "    \"\"\"DirectML-optimized pair creation with reduced data size\"\"\"\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "\n",
    "    if len(person_dirs) > max_people:\n",
    "        person_dirs = np.random.choice(person_dirs, max_people, replace=False)\n",
    "    \n",
    "    print(f\"Using {len(person_dirs)} people for DirectML training\")\n",
    "    pos_anchor_paths = []\n",
    "    pos_comparison_paths = []\n",
    "    neg_anchor_paths = []\n",
    "    neg_comparison_paths = []\n",
    "\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            continue\n",
    "\n",
    "        pair_count = 0\n",
    "        for i in range(len(person_images)):\n",
    "            if pair_count >= max_pairs_per_person:\n",
    "                break\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                if pair_count >= max_pairs_per_person:\n",
    "                    break\n",
    "                pos_anchor_paths.append(person_images[i])\n",
    "                pos_comparison_paths.append(person_images[j])\n",
    "                pair_count += 1\n",
    "\n",
    "    print(f\"Created {len(pos_anchor_paths)} positive pairs\")\n",
    "\n",
    "    target_negative_pairs = len(pos_anchor_paths)\n",
    "    \n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "            break\n",
    "            \n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 1:\n",
    "            continue\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        if not other_people:\n",
    "            continue\n",
    "\n",
    "        for anchor_img in person_images: \n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "\n",
    "            sampled_others = np.random.choice(other_people, min(len(other_people), 10), replace=False)\n",
    "            \n",
    "            for other_person in sampled_others:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(4, len(other_person_images))):\n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                    negative_img = np.random.choice(other_person_images)\n",
    "                    neg_anchor_paths.append(anchor_img)\n",
    "                    neg_comparison_paths.append(negative_img)\n",
    "\n",
    "    print(f\"Created {len(neg_anchor_paths)} negative pairs\")\n",
    "    \n",
    "    all_anchor_paths = pos_anchor_paths + neg_anchor_paths\n",
    "    all_comparison_paths = pos_comparison_paths + neg_comparison_paths\n",
    "\n",
    "    positive_labels = tf.ones(len(pos_anchor_paths))\n",
    "    negative_labels = tf.zeros(len(neg_anchor_paths))\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    return all_anchor_paths, all_comparison_paths, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_data(test_directory, num_pairs=500):\n",
    "    \"\"\"Create validation data (reduced size for DirectML)\"\"\"\n",
    "    person_dirs = [d for d in os.listdir(test_directory) \n",
    "                   if os.path.isdir(os.path.join(test_directory, d))]\n",
    "    \n",
    "    anchors, comparisons, labels = [], [], []\n",
    "    \n",
    "    for person in person_dirs[:30]:  # Reduced for DirectML\n",
    "        person_path = os.path.join(test_directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:5]\n",
    "        \n",
    "        if len(images) >= 2:\n",
    "            for i in range(min(3, len(images)-1)):\n",
    "                anchors.append(images[i])\n",
    "                comparisons.append(images[i+1])\n",
    "                labels.append(1)\n",
    "\n",
    "    for i in range(len(anchors)):\n",
    "        if len(person_dirs) > 1:\n",
    "            other_person = random.choice([p for p in person_dirs[:30] if p != person_dirs[i % len(person_dirs)]])\n",
    "            other_person_path = os.path.join(test_directory, other_person)\n",
    "            other_images = [os.path.join(other_person_path, f) for f in os.listdir(other_person_path) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            if other_images:\n",
    "                anchors.append(anchors[i])\n",
    "                comparisons.append(random.choice(other_images))\n",
    "                labels.append(0)\n",
    "    \n",
    "    return anchors[:num_pairs], comparisons[:num_pairs], labels[:num_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80fcb0",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directml_embedding():\n",
    "    \"\"\"DirectML-optimized embedding network\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(100, 100, 3), name=\"input_img\")\n",
    "\n",
    "    # Simpler architecture for DirectML compatibility\n",
    "    x = tf.keras.layers.Conv2D(32, (5, 5), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(256, activation='sigmoid', dtype='float32')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='directml_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, in_embed, valid_embed):\n",
    "        return tf.math.abs(in_embed - valid_embed)\n",
    "\n",
    "class EuclideanDistance(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        anchor, comparison = inputs\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(anchor - comparison), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af13d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directml_siamese_model():\n",
    "    \"\"\"DirectML-compatible Siamese model\"\"\"\n",
    "    embedding_network = create_directml_embedding()\n",
    "    \n",
    "    anchor_input = Input(shape=(100, 100, 3), name='anchor')\n",
    "    comparison_input = Input(shape=(100, 100, 3), name='comparison')\n",
    "    \n",
    "    anchor_embedding = embedding_network(anchor_input)\n",
    "    comparison_embedding = embedding_network(comparison_input)\n",
    "    \n",
    "    distance = EuclideanDistance(name='euclidean_distance')([anchor_embedding, comparison_embedding])\n",
    "    prediction = Dense(1, activation='sigmoid', name='prediction', dtype='float32')(distance)\n",
    "    \n",
    "    model = Model(inputs=[anchor_input, comparison_input], \n",
    "                  outputs=[distance, prediction],\n",
    "                  name='directml_siamese')\n",
    "    \n",
    "    return model, embedding_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectMLContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # More stable computation for DirectML\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        loss = y_true * tf.square(y_pred) + \\\n",
    "               (1 - y_true) * tf.square(tf.maximum(0.0, self.margin - y_pred))\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directml_model():\n",
    "    \"\"\"Create and compile DirectML-optimized model\"\"\"\n",
    "    siamese_model, embedding_model = create_directml_siamese_model()\n",
    "    \n",
    "    # DirectML-compatible optimizer settings\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-4,  # Slightly higher LR for DirectML\n",
    "        epsilon=1e-7,        # More stable epsilon\n",
    "        clipnorm=1.0         # Gradient clipping for stability\n",
    "    )\n",
    "    \n",
    "    siamese_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            'euclidean_distance': DirectMLContrastiveLoss(margin=1.0),\n",
    "            'prediction': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={'euclidean_distance': 1.0, 'prediction': 0.5},\n",
    "        metrics={'prediction': 'accuracy'}\n",
    "    )\n",
    "    \n",
    "    return siamese_model, embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1eb6cd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef852baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directml_curriculum_training(model, train_directory, test_directory, \n",
    "                               initial_epochs=15, fine_tune_epochs=20, batch_size=32):\n",
    "    \"\"\"DirectML-optimized curriculum training\"\"\"\n",
    "    \n",
    "    print(\"Creating validation data for DirectML training...\")\n",
    "    val_anchors, val_comparisons, val_labels = create_validation_data(test_directory)\n",
    "    \n",
    "    print(\"\\n=== DirectML Stage 1: Initial Training ===\")\n",
    "    \n",
    "    # Reduced data size for DirectML\n",
    "    train_anchors, train_comparisons, train_labels = create_optimized_pairs_from_directory(\n",
    "        train_directory, max_people=800, max_pairs_per_person=25\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(train_anchors)} training pairs for DirectML\")\n",
    "\n",
    "    train_dataset = create_directml_data_pipeline(\n",
    "        train_anchors, train_comparisons, train_labels, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        DirectMLCompatibleEarlyStopping(\n",
    "            validation_data=(val_anchors, val_comparisons, val_labels),\n",
    "            patience=5,\n",
    "            min_verification_acc=0.65,  # Lower threshold for DirectML\n",
    "            verbose=1\n",
    "        ),\n",
    "        DirectMLGPUMonitor(),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.7,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'face_verification_directml.h5',\n",
    "            monitor='loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting DirectML training...\")\n",
    "    history1 = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=initial_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== DirectML Stage 2: Fine-tuning ===\")\n",
    "    \n",
    "    # Reduce learning rate for fine-tuning\n",
    "    model.optimizer.learning_rate = 1e-4\n",
    "    callbacks[0].min_verification_acc = 0.75\n",
    "    callbacks[0].patience = 4\n",
    "    \n",
    "    print(\"Starting DirectML fine-tuning...\")\n",
    "    history2 = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be845a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_directml_face_verification():\n",
    "    \"\"\"Main DirectML training function\"\"\"\n",
    "    \n",
    "    print(\"Creating DirectML-optimized model...\")\n",
    "    model, embedding_model = create_directml_model()\n",
    "    \n",
    "    print(\"Starting DirectML curriculum training...\")\n",
    "    trained_model, training_histories = directml_curriculum_training(\n",
    "        model, TRAIN, TEST, \n",
    "        initial_epochs=20, \n",
    "        fine_tune_epochs=25, \n",
    "        batch_size=24  # Smaller batch size for DirectML\n",
    "    )\n",
    "    \n",
    "    print(\"Saving DirectML model...\")\n",
    "    trained_model.save('face_verification_directml_final.h5')\n",
    "    print(\"DirectML model saved!\")\n",
    "    \n",
    "    return trained_model, training_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341b953",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a934301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting DirectML Face Verification Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if tf.config.list_physical_devices('DML'):\n",
    "    print(f\"DirectML devices available: {len(tf.config.list_physical_devices('DML'))}\")\n",
    "else:\n",
    "    print(\"No DirectML devices found. Training will use CPU.\")\n",
    "\n",
    "try:\n",
    "    final_model, histories = train_directml_face_verification()\n",
    "    print(\"\\n🎉 DirectML training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ DirectML training failed: {e}\")\n",
    "    print(\"Try reducing batch size or model complexity further.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
