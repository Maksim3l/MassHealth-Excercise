{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ebd1e7",
   "metadata": {},
   "source": [
    "# Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52deb349",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea27476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re\n",
    "import uuid\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import shutil\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model(inputs = [inputImg, veriImg], outputs = [1,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.enable_tensor_float_32()\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = os.path.join('data', 'training')\n",
    "TEST = os.path.join('data', 'test')\n",
    "ARCH = os.path.join('data', 'archive')\n",
    "GETPICS = os.path.join(ARCH, 'new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df77a32",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6831fe7",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fe7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    for d in [TRAIN, TEST]:\n",
    "        Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_folders = [\n",
    "        folder for folder in os.listdir(ARCH)\n",
    "        if os.path.isdir(os.path.join(ARCH, folder))\n",
    "    ]\n",
    "\n",
    "    random.shuffle(all_folders)\n",
    "    split_idx = int(len(all_folders) * 0.7)\n",
    "    train_folders = all_folders[:split_idx]\n",
    "    test_folders = all_folders[split_idx:]\n",
    "\n",
    "    for folder_name in train_folders:\n",
    "        src = os.path.join(ARCH, folder_name)\n",
    "        dest = os.path.join(TRAIN, folder_name)\n",
    "        shutil.copytree(src, dest)\n",
    "        print(f\"Copied to training: {folder_name}\")\n",
    "\n",
    "    for folder_name in test_folders:\n",
    "        src = os.path.join(ARCH, folder_name)\n",
    "        dest = os.path.join(TEST, folder_name)\n",
    "        shutil.copytree(src, dest)\n",
    "        print(f\"Copied to testing: {folder_name}\")\n",
    "\n",
    "    print(f\"\\n✅ Done. {len(train_folders)} folders in training, {len(test_folders)} in test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting function because I messed up the top one and did it twice once with folder deleting and once without lol\n",
    "def remove_duplicate_images(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Error: Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    files_to_delete = []\n",
    "    pattern = re.compile(r'^(.+)_1(\\.[^.]+)$')\n",
    "    deleted_count = 0\n",
    "    \n",
    "    print(f\"Scanning directory: {directory}\")\n",
    "\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            extension = match.group(2)\n",
    "            original_file = f\"{base_name}{extension}\"\n",
    "            if original_file in files:\n",
    "                files_to_delete.append(file)\n",
    "\n",
    "    for file in files_to_delete:\n",
    "        try:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nTotal duplicate files deleted: {deleted_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = frame[120:120+250, 200:200+250, :]\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('a'):\n",
    "        img_name = os.path.join(ANC, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(f\"Saved anchor image: {img_name}\")\n",
    "    elif key == ord('p'):\n",
    "        img_name = os.path.join(POS, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(f\"Saved positive image: {img_name}\")\n",
    "    elif key == ord('q'):\n",
    "        print(\"Quitting...\")\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea202e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_from_directory(directory):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    print(f\"Found {len(person_dirs)} people in '{directory}'\")\n",
    "\n",
    "    anchor_paths = []\n",
    "    positive_paths = []\n",
    "    negative_paths = []\n",
    "\n",
    "    total_skipped = 0\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "\n",
    "        for i in range(len(person_images)):\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                anchor_paths.append(person_images[i])\n",
    "                positive_paths.append(person_images[j])\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        for anchor_img in person_images[:10]:\n",
    "            sampled_others = random.sample(other_people, min(len(other_people), 10))  \n",
    "            for other_person in sampled_others:\n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(5, len(other_person_images))):\n",
    "                    negative_img = random.choice(other_person_images)\n",
    "                    anchor_paths.append(anchor_img)\n",
    "                    negative_paths.append(negative_img)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(person_dirs):\n",
    "            print(f\"[{idx + 1}/{len(person_dirs)}] Processed '{person}': \"\n",
    "                  f\"{len(person_images)} imgs, total positives: {len(positive_paths)}, \"\n",
    "                  f\"negatives: {len(negative_paths)}\")\n",
    "\n",
    "    print(f\"\\n✅ Finished pair creation.\")\n",
    "    print(f\"  - Skipped people with <2 images: {total_skipped}\")\n",
    "    print(f\"  - Total positive pairs: {len(positive_paths)}\")\n",
    "    print(f\"  - Total negative pairs: {len(negative_paths)}\")\n",
    "\n",
    "    positive_labels = tf.ones(len(positive_paths))\n",
    "    negative_labels = tf.zeros(len(negative_paths))\n",
    "\n",
    "    all_anchor_paths = anchor_paths + anchor_paths\n",
    "    all_comparison_paths = positive_paths + negative_paths\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    print(f\"  - Final dataset size: {len(all_labels)} pairs (anchors: {len(all_anchor_paths)})\")\n",
    "\n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(all_anchor_paths)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(all_comparison_paths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(all_labels)\n",
    "\n",
    "    return anchor_ds, comparison_ds, labels_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_pairs_from_directory(directory, max_people=1500, max_pairs_per_person=200):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "\n",
    "    if len(person_dirs) > max_people:\n",
    "        person_dirs = np.random.choice(person_dirs, max_people, replace=False)\n",
    "    \n",
    "    print(f\"Using {len(person_dirs)} people (limited from potentially more)\")\n",
    "    pos_anchor_paths = []\n",
    "    pos_comparison_paths = []\n",
    "    neg_anchor_paths = []\n",
    "    neg_comparison_paths = []\n",
    "\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            continue\n",
    "\n",
    "        pair_count = 0\n",
    "        for i in range(len(person_images)):\n",
    "            if pair_count >= max_pairs_per_person:\n",
    "                break\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                if pair_count >= max_pairs_per_person:\n",
    "                    break\n",
    "                pos_anchor_paths.append(person_images[i])\n",
    "                pos_comparison_paths.append(person_images[j])\n",
    "                pair_count += 1\n",
    "\n",
    "    print(f\"Created {len(pos_anchor_paths)} positive pairs\")\n",
    "\n",
    "    target_negative_pairs = len(pos_anchor_paths)\n",
    "    \n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "            break\n",
    "            \n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 1:\n",
    "            continue\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        if not other_people:\n",
    "            continue\n",
    "\n",
    "        for anchor_img in person_images: \n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "\n",
    "            sampled_others = np.random.choice(other_people, min(len(other_people), 20), replace=False)\n",
    "            \n",
    "            for other_person in sampled_others:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(8, len(other_person_images))):  \n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                    negative_img = np.random.choice(other_person_images)\n",
    "                    neg_anchor_paths.append(anchor_img)\n",
    "                    neg_comparison_paths.append(negative_img)\n",
    "\n",
    "    print(f\"Created {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"✅ Final dataset: {len(pos_anchor_paths)} positive, {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"✅ Total pairs: {len(pos_anchor_paths) + len(neg_anchor_paths)}\")\n",
    "    \n",
    "    all_anchor_paths = pos_anchor_paths + neg_anchor_paths\n",
    "    all_comparison_paths = pos_comparison_paths + neg_comparison_paths\n",
    "\n",
    "    positive_labels = tf.ones(len(pos_anchor_paths))\n",
    "    negative_labels = tf.zeros(len(neg_anchor_paths))\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    return all_anchor_paths, all_comparison_paths, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1688e1",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_path):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img,(100,100))\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_preprocess(img_path):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    img = tf.image.resize(img, (100, 100), method='bilinear')\n",
    "    img = tf.cast(img, tf.float16) / 255.0 \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adda82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_twin(in_img, valid_img, label):\n",
    "    return(preprocess(in_img), preprocess(valid_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_preproc_twin(anchor_path, comparison_path, label):\n",
    "    return (\n",
    "        optimized_preprocess(anchor_path), \n",
    "        optimized_preprocess(comparison_path), \n",
    "        tf.cast(label, tf.float16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    anchor_ds, comparison_ds, labels_ds = create_pairs_from_directory(TRAIN)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "    dataset = dataset.map(preproc_twin)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(buffer_size=1024)\n",
    "    \n",
    "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(16)\n",
    "    train_dataset = train_dataset.prefetch(8)\n",
    "    \n",
    "    val_dataset = val_dataset.batch(16)\n",
    "    val_dataset = val_dataset.prefetch(8)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimized_datasets(train_dir, batch_size=32, prefetch_size=tf.data.AUTOTUNE):\n",
    "    anchor_paths, comparison_paths, labels = create_optimized_pairs_from_directory(\n",
    "        train_dir, max_people=1500, max_pairs_per_person=30\n",
    "    )\n",
    "    \n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(anchor_paths)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(comparison_paths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        optimized_preproc_twin, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,  \n",
    "        deterministic=False  \n",
    "    )\n",
    "\n",
    "    dataset = dataset.cache()  \n",
    "    dataset = dataset.shuffle(buffer_size=min(10000, len(anchor_paths)))\n",
    "    dataset_size = len(anchor_paths)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_dataset_generator(directory, max_people=1500, max_pairs_per_person=50):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    person_images = {}\n",
    "    \n",
    "    # Pre-load all image paths\n",
    "    for person in person_dirs:\n",
    "        person_path = os.path.join(directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if len(images) >= 2:\n",
    "            person_images[person] = images\n",
    "    \n",
    "    people_list = list(person_images.keys())\n",
    "    if len(people_list) > max_people:\n",
    "        people_list = np.random.choice(people_list, max_people, replace=False).tolist()\n",
    "        \n",
    "    print(f\"Loaded {len(people_list)} people for dynamic generation\")\n",
    "    \n",
    "    def generate_epoch_dataset(epoch_num):\n",
    "        # Set different random seed for each epoch\n",
    "        np.random.seed(42 + epoch_num)\n",
    "        random.seed(42 + epoch_num)\n",
    "        \n",
    "        pos_anchor_paths = []\n",
    "        pos_comparison_paths = []\n",
    "        neg_anchor_paths = []\n",
    "        neg_comparison_paths = []\n",
    "        \n",
    "        # Generate positive pairs\n",
    "        print(f\"Generating positive pairs for epoch {epoch_num}...\")\n",
    "        for person in people_list:\n",
    "            images = person_images[person]\n",
    "            \n",
    "            # Create positive pairs - sample different combinations each epoch\n",
    "            num_pairs = min(max_pairs_per_person, len(images) * (len(images) - 1) // 2)\n",
    "            \n",
    "            pairs_created = 0\n",
    "            attempts = 0\n",
    "            max_attempts = num_pairs * 3  # Prevent infinite loops\n",
    "            \n",
    "            while pairs_created < num_pairs and attempts < max_attempts:\n",
    "                i = random.randint(0, len(images) - 1)\n",
    "                j = random.randint(0, len(images) - 1)\n",
    "                if i != j:  # Different images\n",
    "                    pos_anchor_paths.append(images[i])\n",
    "                    pos_comparison_paths.append(images[j])\n",
    "                    pairs_created += 1\n",
    "                attempts += 1\n",
    "        \n",
    "        # Generate negative pairs\n",
    "        print(f\"Generating negative pairs for epoch {epoch_num}...\")\n",
    "        target_negative_pairs = len(pos_anchor_paths)  # Match positive pairs\n",
    "        \n",
    "        for person in people_list:\n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "                \n",
    "            images = person_images[person]\n",
    "            other_people = [p for p in people_list if p != person]\n",
    "            \n",
    "            if not other_people:\n",
    "                continue\n",
    "                \n",
    "            # Sample anchor images for this person\n",
    "            anchor_samples = min(10, len(images))\n",
    "            selected_anchors = random.sample(images, anchor_samples)\n",
    "            \n",
    "            for anchor_img in selected_anchors:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Pick random other people for negatives\n",
    "                num_others = min(10, len(other_people))\n",
    "                selected_others = random.sample(other_people, num_others)\n",
    "                \n",
    "                for other_person in selected_others:\n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                        \n",
    "                    other_images = person_images[other_person]\n",
    "                    if other_images:\n",
    "                        negative_img = random.choice(other_images)\n",
    "                        neg_anchor_paths.append(anchor_img)\n",
    "                        neg_comparison_paths.append(negative_img)\n",
    "        \n",
    "        # Balance the dataset\n",
    "        min_size = min(len(pos_anchor_paths), len(neg_anchor_paths))\n",
    "        \n",
    "        # Randomly sample to balance\n",
    "        pos_indices = random.sample(range(len(pos_anchor_paths)), min_size)\n",
    "        neg_indices = random.sample(range(len(neg_anchor_paths)), min_size)\n",
    "        \n",
    "        final_anchors = [pos_anchor_paths[i] for i in pos_indices] + [neg_anchor_paths[i] for i in neg_indices]\n",
    "        final_comparisons = [pos_comparison_paths[i] for i in pos_indices] + [neg_comparison_paths[i] for i in neg_indices]\n",
    "        final_labels = [1.0] * min_size + [0.0] * min_size\n",
    "        \n",
    "        # Shuffle everything together\n",
    "        combined = list(zip(final_anchors, final_comparisons, final_labels))\n",
    "        random.shuffle(combined)\n",
    "        final_anchors, final_comparisons, final_labels = zip(*combined)\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}: Generated {len(final_labels)} pairs ({min_size} pos, {min_size} neg)\")\n",
    "        \n",
    "        return list(final_anchors), list(final_comparisons), list(final_labels)\n",
    "    \n",
    "    return generate_epoch_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c2145",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65e9f2",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeding_make():\n",
    "    in_ = Input(shape=(100,100,3), name=\"in img\")\n",
    "\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(in_)\n",
    "    p1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(p1)\n",
    "    p2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(p2)\n",
    "    p3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(p3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096,activation='sigmoid')(f1)\n",
    "\n",
    "    return Model(inputs=in_, outputs=d1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1da1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_embedding():\n",
    "    inputs = tf.keras.Input(shape=(100, 100, 3), name=\"input_img\")\n",
    "\n",
    "    x = tf.keras.layers.SeparableConv2D(64, (10, 10), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(256, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(512, activation='sigmoid', dtype='float32')(x) \n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='optimized_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bccc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_embedding_with_attention():\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3), name=\"input_img\")  # Larger input size\n",
    "    \n",
    "    # Feature extraction backbone\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "    x = tf.keras.layers.Multiply()([x, attention])\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # L2 normalized embeddings (crucial for face recognition)\n",
    "    embeddings = tf.keras.layers.Dense(512, activation=None, dtype='float32')(x)\n",
    "    embeddings = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(embeddings)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=embeddings, name='advanced_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, in_embed, valid_embed):\n",
    "        return tf.math.abs(in_embed - valid_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e57168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(): #Simanese\n",
    "    input_img = Input(name='input_img', shape=(100,100,3))\n",
    "    validation_img = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    model_layer = L1Dist()\n",
    "    model_layer.name = 'distance'\n",
    "    distances = model_layer(embedding(input_img), embedding(validation_img))\n",
    "\n",
    "    classifier = Dense(1,activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_img, validation_img], outputs=classifier, name='SimaneseNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78180c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred should be [anchor, positive, negative] embeddings\n",
    "        anchor, positive, negative = tf.split(y_pred, 3, axis=1)\n",
    "        \n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        loss = tf.maximum(0.0, pos_dist - neg_dist + self.margin)\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb503e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred is the distance between embeddings\n",
    "        # y_true is 1 for same person, 0 for different\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # For same person pairs, minimize distance\n",
    "        # For different person pairs, maximize distance up to margin\n",
    "        loss = y_true * tf.square(y_pred) + \\\n",
    "               (1 - y_true) * tf.square(tf.maximum(0.0, self.margin - y_pred))\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistance(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        anchor, comparison = inputs\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(anchor - comparison), axis=-1, keepdims=True))\n",
    "\n",
    "def create_advanced_siamese_model():\n",
    "    # Shared embedding network\n",
    "    embedding_network = create_advanced_embedding_with_attention()\n",
    "    \n",
    "    # Input layers\n",
    "    anchor_input = Input(shape=(224, 224, 3), name='anchor')\n",
    "    comparison_input = Input(shape=(224, 224, 3), name='comparison')\n",
    "    \n",
    "    # Get embeddings\n",
    "    anchor_embedding = embedding_network(anchor_input)\n",
    "    comparison_embedding = embedding_network(comparison_input)\n",
    "    \n",
    "    # Calculate distance\n",
    "    distance = EuclideanDistance()([anchor_embedding, comparison_embedding])\n",
    "    \n",
    "    # For contrastive loss, we return the distance directly\n",
    "    # For classification, we can add a dense layer\n",
    "    prediction = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    model = Model(inputs=[anchor_input, comparison_input], \n",
    "                  outputs=[distance, prediction],  # Return both distance and prediction\n",
    "                  name='advanced_siamese')\n",
    "    \n",
    "    return model, embedding_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d85549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_augmentation():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "        tf.keras.layers.RandomBrightness(0.1),\n",
    "    ])\n",
    "\n",
    "def preprocess_with_augmentation(img_path, is_training=True):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    img = tf.image.resize(img, (224, 224))  # Larger input size\n",
    "    \n",
    "    # Face detection and cropping would be ideal here\n",
    "    # For now, we'll assume faces are centered\n",
    "    \n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if is_training:\n",
    "        # Apply augmentation only during training\n",
    "        augmentation = advanced_augmentation()\n",
    "        img = augmentation(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# 6. ADVANCED TRAINING WITH MULTIPLE LOSSES\n",
    "def create_multi_loss_model():\n",
    "    siamese_model, embedding_model = create_advanced_siamese_model()\n",
    "    \n",
    "    # Compile with multiple losses\n",
    "    siamese_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            'euclidean_distance': ContrastiveLoss(margin=1.0),\n",
    "            'dense': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={'euclidean_distance': 1.0, 'dense': 0.5},\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return siamese_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96043ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fb627",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_optimized_embedding()\n",
    "siamese_model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f69853",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoints, 'ckpt2')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def t_step(batch, model, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:  \n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        y_pred = model([anchor_img, comparison_img], training=True)\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_train_step(batch, model, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        y_pred = model([anchor_img, comparison_img], training=True)\n",
    "        \n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss) if hasattr(optimizer, 'get_scaled_loss') else loss\n",
    "\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    if hasattr(optimizer, 'get_unscaled_gradients'):\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    else:\n",
    "        gradients = scaled_gradients\n",
    "    gradients = [tf.clip_by_norm(grad, 1.0) if grad is not None else grad for grad in gradients]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, epochs))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))  \n",
    "        for idx, batch in enumerate(data): \n",
    "            t_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9656e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, epochs=50):\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        \n",
    "        progress_bar = tf.keras.utils.Progbar(len(train_dataset))\n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            loss = t_step(batch, model, opt, binary_cross_loss)\n",
    "            \n",
    "            train_loss(loss)\n",
    "            train_accuracy(batch[2], model([batch[0], batch[1]], training=False))\n",
    "            progress_bar.update(batch_idx + 1)\n",
    "\n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(batch[2], val_preds)\n",
    "\n",
    "        print(f\"Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result():.4f}, Val Accuracy: {val_accuracy.result():.4f}\")\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_retrain_model(model, train_dataset, val_dataset, epochs=10):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    if hasattr(tf.keras.mixed_precision, 'LossScaleOptimizer'):\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "   \n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "    best_val_acc = 0\n",
    "    patience = 3\n",
    "    wait = 0\n",
    "    best_weights_saved = False\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        num_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "        progress_bar = tf.keras.utils.Progbar(num_batches)\n",
    "       \n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            loss = optimized_train_step(batch, model, optimizer, loss_fn)\n",
    "            train_loss(loss)\n",
    "            preds = model([batch[0], batch[1]], training=False)\n",
    "            train_accuracy(tf.cast(batch[2], tf.float32), tf.cast(preds, tf.float32))\n",
    "           \n",
    "            progress_bar.update(batch_idx + 1)\n",
    "       \n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(tf.cast(batch[2], tf.float32), tf.cast(val_preds, tf.float32))\n",
    "       \n",
    "        current_val_acc = val_accuracy.result()\n",
    "        print(f\"Loss: {train_loss.result():.4f}, \"\n",
    "              f\"Accuracy: {train_accuracy.result():.4f}, \"\n",
    "              f\"Val Accuracy: {current_val_acc:.4f}\")\n",
    "        \n",
    "        if current_val_acc > best_val_acc:\n",
    "            best_val_acc = current_val_acc\n",
    "            wait = 0\n",
    "            model.save_weights('best_model.weights.h5')\n",
    "            best_weights_saved = True\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.4f} - weights saved\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_weights_saved:\n",
    "        model.load_weights('best_model.weights.h5')\n",
    "        print(\"Loaded best weights\")\n",
    "    else:\n",
    "        print(\"No improvement found, keeping current weights\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33407c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_dataset_generator(directory, max_people=1500, max_pairs_per_person=50):\n",
    "    \"\"\"\n",
    "    Creates a generator that produces different dataset each epoch\n",
    "    \"\"\"\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    person_images = {}\n",
    "    \n",
    "    # Pre-load all image paths\n",
    "    for person in person_dirs:\n",
    "        person_path = os.path.join(directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if len(images) >= 2:\n",
    "            person_images[person] = images\n",
    "    \n",
    "    people_list = list(person_images.keys())\n",
    "    if len(people_list) > max_people:\n",
    "        people_list = np.random.choice(people_list, max_people, replace=False).tolist()\n",
    "        \n",
    "    print(f\"Loaded {len(people_list)} people for dynamic generation\")\n",
    "    \n",
    "    def generate_epoch_dataset(epoch_num):\n",
    "        # Set different random seed for each epoch\n",
    "        np.random.seed(42 + epoch_num)\n",
    "        random.seed(42 + epoch_num)\n",
    "        \n",
    "        pos_anchor_paths = []\n",
    "        pos_comparison_paths = []\n",
    "        neg_anchor_paths = []\n",
    "        neg_comparison_paths = []\n",
    "        \n",
    "        # Generate positive pairs\n",
    "        print(f\"Generating positive pairs for epoch {epoch_num}...\")\n",
    "        for person in people_list:\n",
    "            images = person_images[person]\n",
    "            \n",
    "            # Create positive pairs - sample different combinations each epoch\n",
    "            num_pairs = min(max_pairs_per_person, len(images) * (len(images) - 1) // 2)\n",
    "            \n",
    "            pairs_created = 0\n",
    "            attempts = 0\n",
    "            max_attempts = num_pairs * 3  # Prevent infinite loops\n",
    "            \n",
    "            while pairs_created < num_pairs and attempts < max_attempts:\n",
    "                i = random.randint(0, len(images) - 1)\n",
    "                j = random.randint(0, len(images) - 1)\n",
    "                if i != j:  # Different images\n",
    "                    pos_anchor_paths.append(images[i])\n",
    "                    pos_comparison_paths.append(images[j])\n",
    "                    pairs_created += 1\n",
    "                attempts += 1\n",
    "        \n",
    "        # Generate negative pairs\n",
    "        print(f\"Generating negative pairs for epoch {epoch_num}...\")\n",
    "        target_negative_pairs = len(pos_anchor_paths)  # Match positive pairs\n",
    "        \n",
    "        for person in people_list:\n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "                \n",
    "            images = person_images[person]\n",
    "            other_people = [p for p in people_list if p != person]\n",
    "            \n",
    "            if not other_people:\n",
    "                continue\n",
    "                \n",
    "            # Sample anchor images for this person\n",
    "            anchor_samples = min(10, len(images))\n",
    "            selected_anchors = random.sample(images, anchor_samples)\n",
    "            \n",
    "            for anchor_img in selected_anchors:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Pick random other people for negatives\n",
    "                num_others = min(10, len(other_people))\n",
    "                selected_others = random.sample(other_people, num_others)\n",
    "                \n",
    "                for other_person in selected_others:\n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                        \n",
    "                    other_images = person_images[other_person]\n",
    "                    if other_images:\n",
    "                        negative_img = random.choice(other_images)\n",
    "                        neg_anchor_paths.append(anchor_img)\n",
    "                        neg_comparison_paths.append(negative_img)\n",
    "        \n",
    "        # Balance the dataset\n",
    "        min_size = min(len(pos_anchor_paths), len(neg_anchor_paths))\n",
    "        \n",
    "        # Randomly sample to balance\n",
    "        pos_indices = random.sample(range(len(pos_anchor_paths)), min_size)\n",
    "        neg_indices = random.sample(range(len(neg_anchor_paths)), min_size)\n",
    "        \n",
    "        final_anchors = [pos_anchor_paths[i] for i in pos_indices] + [neg_anchor_paths[i] for i in neg_indices]\n",
    "        final_comparisons = [pos_comparison_paths[i] for i in pos_indices] + [neg_comparison_paths[i] for i in neg_indices]\n",
    "        final_labels = [1.0] * min_size + [0.0] * min_size\n",
    "        \n",
    "        # Shuffle everything together\n",
    "        combined = list(zip(final_anchors, final_comparisons, final_labels))\n",
    "        random.shuffle(combined)\n",
    "        final_anchors, final_comparisons, final_labels = zip(*combined)\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}: Generated {len(final_labels)} pairs ({min_size} pos, {min_size} neg)\")\n",
    "        \n",
    "        return list(final_anchors), list(final_comparisons), list(final_labels)\n",
    "    \n",
    "    return generate_epoch_dataset\n",
    "\n",
    "def train_with_dynamic_datasets(model, train_directory, test_directory, epochs=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Training loop with dynamic dataset generation + fixed validation\n",
    "    \"\"\"\n",
    "    # Simple, robust training step function\n",
    "    @tf.function\n",
    "    def simple_train_step(batch, model, optimizer, loss_fn):\n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model([anchor_img, comparison_img], training=True)\n",
    "            y_pred = tf.cast(y_pred, tf.float32)\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            loss = loss_fn(y_true, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        gradients = [tf.clip_by_norm(grad, 1.0) if grad is not None else grad for grad in gradients]\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Dynamic training data generator\n",
    "    train_generator = create_dynamic_dataset_generator(train_directory, max_people=1500, max_pairs_per_person=30)\n",
    "    \n",
    "    # Fixed validation dataset (created once, reused every epoch)\n",
    "    print(\"Creating fixed validation dataset...\")\n",
    "    val_anchors, val_comparisons, val_labels = create_optimized_pairs_from_directory(\n",
    "        test_directory, max_people=300, max_pairs_per_person=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation dataset: {len(val_labels)} pairs\")\n",
    "    \n",
    "    # Create validation TensorFlow dataset\n",
    "    val_anchor_ds = tf.data.Dataset.from_tensor_slices(val_anchors)\n",
    "    val_comparison_ds = tf.data.Dataset.from_tensor_slices(val_comparisons)\n",
    "    val_labels_ds = tf.data.Dataset.from_tensor_slices(val_labels)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.zip((val_anchor_ds, val_comparison_ds, val_labels_ds))\n",
    "    val_dataset = val_dataset.map(optimized_preproc_twin, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 5\n",
    "    wait = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "        \n",
    "        # Generate NEW training dataset for this epoch\n",
    "        anchors, comparisons, labels = train_generator(epoch)\n",
    "        print(f\"Training dataset: {len(labels)} pairs\")\n",
    "        \n",
    "        # Create TensorFlow dataset for this epoch\n",
    "        anchor_ds = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "        comparison_ds = tf.data.Dataset.from_tensor_slices(comparisons)\n",
    "        labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "        train_dataset = train_dataset.map(optimized_preproc_twin, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        \n",
    "        num_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "        progress_bar = tf.keras.utils.Progbar(num_batches)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            # Use the simple training step\n",
    "            loss = simple_train_step(batch, model, optimizer, loss_fn)\n",
    "            \n",
    "            # Ensure loss is not None and is a proper tensor\n",
    "            if loss is not None:\n",
    "                train_loss(loss)\n",
    "                \n",
    "                preds = model([batch[0], batch[1]], training=False)\n",
    "                train_accuracy(tf.cast(batch[2], tf.float32), tf.cast(preds, tf.float32))\n",
    "            else:\n",
    "                print(f\"Warning: Loss is None at batch {batch_idx}\")\n",
    "            \n",
    "            progress_bar.update(batch_idx + 1)\n",
    "        \n",
    "        # Validation phase (using FIXED validation set)\n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(tf.cast(batch[2], tf.float32), tf.cast(val_preds, tf.float32))\n",
    "        \n",
    "        current_val_acc = val_accuracy.result()\n",
    "        print(f\"Loss: {train_loss.result():.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy.result():.4f}, \"\n",
    "              f\"Val Acc: {current_val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if current_val_acc > best_val_acc:\n",
    "            best_val_acc = current_val_acc\n",
    "            wait = 0\n",
    "            model.save_weights('best_dynamic_model.weights.h5')\n",
    "            print(f\"🎯 New best validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save_weights(f'dynamic_model_epoch_{epoch+1}.weights.h5')\n",
    "    \n",
    "    # Load best weights\n",
    "    if os.path.exists('best_dynamic_model.weights.h5'):\n",
    "        model.load_weights('best_dynamic_model.weights.h5')\n",
    "        print(f\"✅ Training complete. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ec2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(train_data, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain_model(siamese_model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b789ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = prepare_optimized_datasets(\n",
    "    TRAIN, \n",
    "    batch_size=64,\n",
    "    prefetch_size=tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1748dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_model = optimized_retrain_model(\n",
    "#    siamese_model, \n",
    "#    train_dataset, \n",
    "#    val_dataset, \n",
    "#    epochs=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad790596",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save('face_verification_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29346b68",
   "metadata": {},
   "source": [
    "### Test / Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "y_hat = siamese_model.predict([test_in, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e706660",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Recall()\n",
    "m.update_state(y_true, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Precision()\n",
    "m.update_state(y_true, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_in[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('face_verification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('face_verification.h5', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ce63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_two_images(img1_path, img2_path, model):\n",
    "    def preprocess_single_image(img_path):\n",
    "        byte_img = tf.io.read_file(img_path)\n",
    "        img = tf.io.decode_jpeg(byte_img)\n",
    "        img = tf.image.resize(img, (100, 100))\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "\n",
    "    img1 = preprocess_single_image(img1_path)\n",
    "    img2 = preprocess_single_image(img2_path)\n",
    "\n",
    "    # Add batch dimension\n",
    "    img1 = tf.expand_dims(img1, axis=0)\n",
    "    img2 = tf.expand_dims(img2, axis=0)\n",
    "\n",
    "    # Predict similarity\n",
    "    result = model.predict([img1, img2])\n",
    "    print(f\"Similarity score: {result[0][0]:.4f}\")\n",
    "    \n",
    "    if result[0][0] > 0.5:\n",
    "        print(\"✅ Match: Likely the same person\")\n",
    "    else:\n",
    "        print(\"❌ No Match: Likely different people\")\n",
    "\n",
    "test_on_two_images('C:/Users/lokna/Projects/MyReactNativeApp/extensions/face_auth/data/positive/496e69d0-3499-11f0-a4ad-e8fb1c79b654.jpg', 'C:/Users/lokna/Projects/MyReactNativeApp/extensions/face_auth/data/positive/4a5df055-3499-11f0-ae89-e8fb1c79b654.jpg', model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
