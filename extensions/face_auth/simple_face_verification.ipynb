{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68490b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import pillow_heif\n",
    "# \n",
    "# # pillow_heif.register_heif_opener()\n",
    "# \n",
    "# def heic_to_jpg(input_path, output_path):\n",
    "#     image = Image.open(input_path)\n",
    "#     image.save(output_path, \"JPEG\")\n",
    "# \n",
    "# def delete_all_heic_in_folder(folder_path):\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         print(f\"Error: Folder '{folder_path}' does not exist.\")\n",
    "#         return\n",
    "# \n",
    "#     count = 0\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.lower().endswith(\".heic\"):\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "#             try:\n",
    "#                 os.remove(file_path)\n",
    "#                 print(f\"Deleted: {filename}\")\n",
    "#                 count += 1\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to delete {filename}: {e}\")\n",
    "#     \n",
    "#     print(f\"Total .heic files deleted: {count}\")\n",
    "# \n",
    "# def convert_all_heic_in_folder(input_folder, output_folder=None):\n",
    "#     if not os.path.isdir(input_folder):\n",
    "#         print(f\"Error: Input folder '{input_folder}' does not exist.\")\n",
    "#         return\n",
    "# \n",
    "#     if output_folder is None:\n",
    "#         output_folder = input_folder\n",
    "# \n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "# \n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if filename.lower().endswith(\".heic\"):\n",
    "#             input_path = os.path.join(input_folder, filename)\n",
    "#             output_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "#             output_path = os.path.join(output_folder, output_filename)\n",
    "# \n",
    "#             try:\n",
    "#                 heic_to_jpg(input_path, output_path)\n",
    "#                 print(f\"Converted: {filename} â†’ {output_filename}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to convert {filename}: {e}\")\n",
    "# \n",
    "# # delete_all_heic_in_folder(\"./indep_data/kuharic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80553449",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0d8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokna\\Projects\\MyReactNativeApp\\extensions\\face_auth\\pytorch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f6d5f",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f92f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f05e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealVGGFace2Model(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained='vggface2'):\n",
    "        super(RealVGGFace2Model, self).__init__()\n",
    "        self.backbone = InceptionResnetV1(pretrained=pretrained)\n",
    "        feature_dim = 512\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.backbone(x)\n",
    "        return embeddings\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2508c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vggface2_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1] normalization\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2111b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face_dataset(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    \n",
    "    for idx, person_folder in enumerate(sorted(os.listdir(data_dir))):\n",
    "        person_path = os.path.join(data_dir, person_folder)\n",
    "        if os.path.isdir(person_path):\n",
    "            class_names.append(person_folder)\n",
    "            for img_file in os.listdir(person_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(person_path, img_file))\n",
    "                    labels.append(idx)\n",
    "    \n",
    "    return image_paths, labels, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2739cdf",
   "metadata": {},
   "source": [
    "# Model Help Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec9b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vggface2_model(model, train_loader, val_loader, num_epochs=20, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    print(\"Phase 1: Training classifier only (backbone frozen)\")\n",
    "    train_losses, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs // 2):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs//2}], Loss: {epoch_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        scheduler.step()\n",
    "    print(\"\\nPhase 2: Fine-tuning entire model (backbone unfrozen)\")\n",
    "    model.unfreeze_backbone()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr/10, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    for epoch in range(num_epochs // 2, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        scheduler.step()\n",
    "    \n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e99ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_faces_vggface2(model, img1_path, img2_path, threshold=0.6):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    _, val_transform = get_vggface2_transforms()\n",
    "    \n",
    "    try:\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "        \n",
    "        img1_tensor = val_transform(img1).unsqueeze(0).to(device)\n",
    "        img2_tensor = val_transform(img2).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb1 = model.get_embeddings(img1_tensor)\n",
    "            emb2 = model.get_embeddings(img2_tensor)\n",
    "            similarity = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
    "            similarity_score = similarity.item()\n",
    "            \n",
    "            is_same_person = similarity_score > threshold\n",
    "            \n",
    "        return is_same_person, similarity_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing images: {e}\")\n",
    "        return False, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c942a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_vggface2_training():\n",
    "    DATA_DIR = \"./indep_data\"\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    image_paths, labels, class_names = load_face_dataset(DATA_DIR)\n",
    "    print(f\"Found {len(image_paths)} images across {len(class_names)} people\")\n",
    "    print(f\"People: {class_names}\")\n",
    "    \n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_transform, val_transform = get_vggface2_transforms()\n",
    "    train_dataset = FaceDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = FaceDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    model = RealVGGFace2Model(num_classes=num_classes, pretrained='vggface2')\n",
    "    \n",
    "    print(f\"Training REAL VGG-Face2 model with {num_classes} classes...\")\n",
    "    print(\"This model is actually pretrained on VGG-Face2 dataset!\")\n",
    "    \n",
    "    train_losses, val_accuracies = train_vggface2_model(\n",
    "        model, train_loader, val_loader, \n",
    "        num_epochs=NUM_EPOCHS, lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_names': class_names,\n",
    "        'num_classes': num_classes\n",
    "    }, 'real_vggface2_model.pth')\n",
    "    \n",
    "    print(\"REAL VGG-Face2 model saved as 'real_vggface2_model.pth'\")\n",
    "    \n",
    "    return model, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953ce013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_vggface2_model():\n",
    "    checkpoint = torch.load('real_vggface2_model.pth', map_location='cpu')\n",
    "    \n",
    "    num_classes = checkpoint['num_classes']\n",
    "    class_names = checkpoint['class_names']\n",
    "    \n",
    "    model = RealVGGFace2Model(num_classes=num_classes, pretrained='vggface2')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb1809",
   "metadata": {},
   "source": [
    "# Running it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd7bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 276 images across 2 people\n",
      "People: ['kuharic', 'loknar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107M/107M [00:43<00:00, 2.58MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training REAL VGG-Face2 model with 2 classes...\n",
      "This model is actually pretrained on VGG-Face2 dataset!\n",
      "Phase 1: Training classifier only (backbone frozen)\n",
      "Epoch [1/10], Loss: 0.5895, Val Acc: 80.36%\n",
      "Epoch [2/10], Loss: 0.3275, Val Acc: 91.07%\n",
      "Epoch [3/10], Loss: 0.2493, Val Acc: 91.07%\n",
      "Epoch [4/10], Loss: 0.1919, Val Acc: 98.21%\n",
      "Epoch [5/10], Loss: 0.1885, Val Acc: 94.64%\n",
      "Epoch [6/10], Loss: 0.1496, Val Acc: 98.21%\n",
      "Epoch [7/10], Loss: 0.1908, Val Acc: 87.50%\n",
      "Epoch [8/10], Loss: 0.2038, Val Acc: 94.64%\n",
      "Epoch [9/10], Loss: 0.1477, Val Acc: 94.64%\n",
      "Epoch [10/10], Loss: 0.1349, Val Acc: 94.64%\n",
      "\n",
      "Phase 2: Fine-tuning entire model (backbone unfrozen)\n",
      "Epoch [11/20], Loss: 0.1091, Val Acc: 100.00%\n",
      "Epoch [12/20], Loss: 0.0497, Val Acc: 100.00%\n",
      "Epoch [13/20], Loss: 0.0639, Val Acc: 94.64%\n",
      "Epoch [14/20], Loss: 0.1254, Val Acc: 100.00%\n",
      "Epoch [15/20], Loss: 0.0225, Val Acc: 98.21%\n",
      "Epoch [16/20], Loss: 0.0108, Val Acc: 100.00%\n",
      "Epoch [17/20], Loss: 0.0393, Val Acc: 100.00%\n",
      "Epoch [18/20], Loss: 0.0232, Val Acc: 100.00%\n",
      "Epoch [19/20], Loss: 0.0115, Val Acc: 100.00%\n",
      "Epoch [20/20], Loss: 0.0757, Val Acc: 100.00%\n",
      "REAL VGG-Face2 model saved as 'real_vggface2_model.pth'\n"
     ]
    }
   ],
   "source": [
    "model, class_names = main_vggface2_training()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# img1_path = \"./indep_data/person1/photo1.jpg\"\n",
    "# img2_path = \"./indep_data/person2/photo1.jpg\"\n",
    "# is_same, similarity = compare_faces_vggface2(model, img1_path, img2_path)\n",
    "# print(f\"Same person: {is_same}, VGG-Face2 similarity: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a8afb",
   "metadata": {},
   "source": [
    "# Example usage for face verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408e2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_vggface2_comparison(img1_path, img2_path, threshold=0.6):\n",
    "    model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "    \n",
    "    _, transform = get_vggface2_transforms()\n",
    "\n",
    "    img1 = transform(Image.open(img1_path).convert('RGB')).unsqueeze(0)\n",
    "    img2 = transform(Image.open(img2_path).convert('RGB')).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb1 = model(img1)\n",
    "        emb2 = model(img2)\n",
    "        \n",
    "        similarity = torch.nn.functional.cosine_similarity(emb1, emb2).item()\n",
    "        is_same = similarity > threshold\n",
    "    \n",
    "    return is_same, similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62aa57",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def quick_vggface2_compare(img1_path, img2_path):\n",
    "    \"\"\"Quick comparison using pure VGG-Face2 (no training needed)\"\"\"\n",
    "    is_same, similarity = direct_vggface2_comparison(img1_path, img2_path)\n",
    "    print(f\"VGG-Face2 comparison:\")\n",
    "    print(f\"Same person: {is_same}\")\n",
    "    print(f\"Similarity: {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
