{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80553449",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363824f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f6d5f",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f92f5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0ba69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c4ab4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_face_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Assumes directory structure:\n",
    "    data_dir/\n",
    "    ├── person1/\n",
    "    │   ├── img1.jpg\n",
    "    │   ├── img2.jpg\n",
    "    │   └── ...\n",
    "    ├── person2/\n",
    "    │   ├── img1.jpg\n",
    "    │   └── ...\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    \n",
    "    for idx, person_folder in enumerate(sorted(os.listdir(data_dir))):\n",
    "        person_path = os.path.join(data_dir, person_folder)\n",
    "        if os.path.isdir(person_path):\n",
    "            class_names.append(person_folder)\n",
    "            for img_file in os.listdir(person_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(person_path, img_file))\n",
    "                    labels.append(idx)\n",
    "    \n",
    "    return image_paths, labels, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2739cdf",
   "metadata": {},
   "source": [
    "# Model Help Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc154239",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VGGFace2Model(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(VGGFace2Model, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-50\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Add custom classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Extract feature embeddings without classification\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.classifier[:-1](features)  # Exclude final linear layer\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c5e50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=30, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb1809",
   "metadata": {},
   "source": [
    "# Running it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d4938",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def verify_faces(model, img1_path, img2_path, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Verify if two face images belong to the same person\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    _, val_transform = get_transforms()\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    img1 = Image.open(img1_path).convert('RGB')\n",
    "    img2 = Image.open(img2_path).convert('RGB')\n",
    "    \n",
    "    img1_tensor = val_transform(img1).unsqueeze(0).to(device)\n",
    "    img2_tensor = val_transform(img2).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        emb1 = model.get_embeddings(img1_tensor)\n",
    "        emb2 = model.get_embeddings(img2_tensor)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
    "        \n",
    "        is_same_person = similarity.item() > threshold\n",
    "        \n",
    "    return is_same_person, similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8183c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    DATA_DIR = \"path/to/your/face/dataset\"  # Update this path\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    image_paths, labels, class_names = load_face_dataset(DATA_DIR)\n",
    "    print(f\"Found {len(image_paths)} images across {len(class_names)} people\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_transform, val_transform = get_transforms()\n",
    "    train_dataset = FaceDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = FaceDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    num_classes = len(class_names)\n",
    "    model = VGGFace2Model(num_classes=num_classes)\n",
    "    \n",
    "    print(f\"Training model with {num_classes} classes...\")\n",
    "\n",
    "    train_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        num_epochs=NUM_EPOCHS, lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    torch.save(model.state_dict(), 'vggface2_finetuned.pth')\n",
    "    print(\"Model saved as 'vggface2_finetuned.pth'\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a8afb",
   "metadata": {},
   "source": [
    "# Example usage for face verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bdf76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load trained model\n",
    "model = VGGFace2Model(num_classes=2)  # Adjust based on your number of people\n",
    "model.load_state_dict(torch.load('vggface2_finetuned.pth'))\n",
    "\n",
    "# Verify two faces\n",
    "is_same, similarity = verify_faces(model, 'path/to/img1.jpg', 'path/to/img2.jpg')\n",
    "print(f\"Same person: {is_same}, Similarity: {similarity:.3f}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
