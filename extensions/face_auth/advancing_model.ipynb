{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def57a3e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b48e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re\n",
    "import uuid\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import shutil\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.callbacks import *\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d1e7d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_policy('mixed_float16')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.enable_tensor_float_32()\n",
    "        tf.config.set_soft_device_placement(True)\n",
    "        tf.debugging.set_log_device_placement(False)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fabcef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3a2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = os.path.join('data', 'training')\n",
    "TEST = os.path.join('data', 'test')\n",
    "ARCH = os.path.join('data', 'archive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25078d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationAccuracyEarlyStopping(Callback):\n",
    "    \"\"\"Custom early stopping based on verification accuracy degradation\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data=None, patience=5, min_delta=0.01, \n",
    "                 min_verification_acc=0.90, restore_best_weights=True, verbose=1):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_verification_acc = min_verification_acc\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        self.best_verification_acc = 0\n",
    "        self.best_weights = None\n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            val_acc = self.calculate_verification_accuracy()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch + 1} - Verification Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > self.best_verification_acc + self.min_delta:\n",
    "                self.best_verification_acc = val_acc\n",
    "                self.wait = 0\n",
    "                if self.restore_best_weights:\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if val_acc < self.min_verification_acc:\n",
    "                print(f\"Verification accuracy dropped below {self.min_verification_acc}. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Verification accuracy hasn't improved for {self.patience} epochs. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "            if self.model.stop_training and self.restore_best_weights and self.best_weights:\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                print(f\"Restored best weights with verification accuracy: {self.best_verification_acc:.4f}\")\n",
    "    \n",
    "    def calculate_verification_accuracy(self):\n",
    "        \"\"\"Calculate verification accuracy on validation pairs\"\"\"\n",
    "        anchor_imgs, comparison_imgs, labels = self.validation_data\n",
    "        \n",
    "        batch_size = 32\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(anchor_imgs), batch_size):\n",
    "            try:\n",
    "                batch_anchors = anchor_imgs[i:i+batch_size]\n",
    "                batch_comparisons = comparison_imgs[i:i+batch_size]\n",
    "                \n",
    "                processed_anchors = []\n",
    "                processed_comparisons = []\n",
    "                \n",
    "                for img in batch_anchors:\n",
    "                    if isinstance(img, str):\n",
    "                        processed_img = preprocess_with_augmentation(img, is_training=False)\n",
    "                    else:\n",
    "                        processed_img = tf.cast(img, tf.float32) / 255.0 if tf.reduce_max(img) > 1.0 else img\n",
    "                    processed_anchors.append(processed_img)\n",
    "                \n",
    "                for img in batch_comparisons:\n",
    "                    if isinstance(img, str):\n",
    "                        processed_img = preprocess_with_augmentation(img, is_training=False)\n",
    "                    else:\n",
    "                        processed_img = tf.cast(img, tf.float32) / 255.0 if tf.reduce_max(img) > 1.0 else img\n",
    "                    processed_comparisons.append(processed_img)\n",
    "                \n",
    "                processed_anchors = tf.stack(processed_anchors)\n",
    "                processed_comparisons = tf.stack(processed_comparisons)\n",
    "                \n",
    "                # Handle both single and multi-output models\n",
    "                model_output = self.model([processed_anchors, processed_comparisons])\n",
    "                if isinstance(model_output, list):\n",
    "                    batch_preds = model_output[-1]  # Use the prediction output\n",
    "                else:\n",
    "                    batch_preds = model_output\n",
    "                    \n",
    "                predictions.extend(batch_preds.numpy().flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch processing: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        binary_preds = (np.array(predictions) > 0.5).astype(int)\n",
    "        return accuracy_score(labels[:len(binary_preds)], binary_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b6c5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUUtilizationMonitor(Callback):\n",
    "    \"\"\"Monitor and optimize GPU utilization during training\"\"\"\n",
    "    \n",
    "    def __init__(self, target_utilization=0.95):\n",
    "        super().__init__()\n",
    "        self.target_utilization = target_utilization\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e56542",
   "metadata": {},
   "source": [
    "# Datamanipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353488e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_pairs_from_directory(directory, max_people=1500, max_pairs_per_person=200):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "\n",
    "    if len(person_dirs) > max_people:\n",
    "        person_dirs = np.random.choice(person_dirs, max_people, replace=False)\n",
    "    \n",
    "    print(f\"Using {len(person_dirs)} people (limited from potentially more)\")\n",
    "    pos_anchor_paths = []\n",
    "    pos_comparison_paths = []\n",
    "    neg_anchor_paths = []\n",
    "    neg_comparison_paths = []\n",
    "\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            continue\n",
    "\n",
    "        pair_count = 0\n",
    "        for i in range(len(person_images)):\n",
    "            if pair_count >= max_pairs_per_person:\n",
    "                break\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                if pair_count >= max_pairs_per_person:\n",
    "                    break\n",
    "                pos_anchor_paths.append(person_images[i])\n",
    "                pos_comparison_paths.append(person_images[j])\n",
    "                pair_count += 1\n",
    "\n",
    "    print(f\"Created {len(pos_anchor_paths)} positive pairs\")\n",
    "\n",
    "    target_negative_pairs = len(pos_anchor_paths)\n",
    "    \n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "            break\n",
    "            \n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 1:\n",
    "            continue\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        if not other_people:\n",
    "            continue\n",
    "\n",
    "        for anchor_img in person_images: \n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "\n",
    "            sampled_others = np.random.choice(other_people, min(len(other_people), 20), replace=False)\n",
    "            \n",
    "            for other_person in sampled_others:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(8, len(other_person_images))):  \n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                    negative_img = np.random.choice(other_person_images)\n",
    "                    neg_anchor_paths.append(anchor_img)\n",
    "                    neg_comparison_paths.append(negative_img)\n",
    "\n",
    "    print(f\"Created {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"âœ… Final dataset: {len(pos_anchor_paths)} positive, {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"âœ… Total pairs: {len(pos_anchor_paths) + len(neg_anchor_paths)}\")\n",
    "    \n",
    "    all_anchor_paths = pos_anchor_paths + neg_anchor_paths\n",
    "    all_comparison_paths = pos_comparison_paths + neg_comparison_paths\n",
    "\n",
    "    positive_labels = tf.ones(len(pos_anchor_paths))\n",
    "    negative_labels = tf.zeros(len(neg_anchor_paths))\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    return all_anchor_paths, all_comparison_paths, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a76158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_augmentation():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "        tf.keras.layers.RandomBrightness(0.1),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d97820e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_AUGMENTATION = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.RandomBrightness(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c05ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_augmentation(img_path, is_training=True):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    img = tf.image.resize(img, (100, 100))  # Larger input size\n",
    "    \n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if is_training:\n",
    "        img = GLOBAL_AUGMENTATION(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "925b4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hard_negative_pairs(embedding_model, directory, num_hard_negatives=1000):\n",
    "    \"\"\"Create pairs where the model currently struggles (hard negatives)\"\"\"\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    \n",
    "    all_embeddings = {}\n",
    "    all_image_paths = {}\n",
    "    \n",
    "    for person in person_dirs[:100]: \n",
    "        person_path = os.path.join(directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:10]\n",
    "        \n",
    "        if len(images) < 2:\n",
    "            continue\n",
    "            \n",
    "        embeddings = []\n",
    "        for img_path in images:\n",
    "            img = preprocess_with_augmentation(img_path, is_training=False)\n",
    "            img = tf.expand_dims(img, 0)\n",
    "            emb = embedding_model(img)\n",
    "            embeddings.append(emb.numpy()[0])\n",
    "        \n",
    "        all_embeddings[person] = np.array(embeddings)\n",
    "        all_image_paths[person] = images\n",
    "\n",
    "    hard_anchor_paths = []\n",
    "    hard_negative_paths = []\n",
    "    \n",
    "    people_list = list(all_embeddings.keys())\n",
    "    \n",
    "    for i, person1 in enumerate(people_list):\n",
    "        for j, person2 in enumerate(people_list[i+1:], i+1):\n",
    "            # Calculate all pairwise distances between person1 and person2\n",
    "            for k, emb1 in enumerate(all_embeddings[person1]):\n",
    "                for l, emb2 in enumerate(all_embeddings[person2]):\n",
    "                    distance = np.linalg.norm(emb1 - emb2)\n",
    "                    \n",
    "                    if distance < 0.5:  # Threshold for \"hard negative\"\n",
    "                        hard_anchor_paths.append(all_image_paths[person1][k])\n",
    "                        hard_negative_paths.append(all_image_paths[person2][l])\n",
    "                        \n",
    "                        if len(hard_anchor_paths) >= num_hard_negatives:\n",
    "                            return hard_anchor_paths, hard_negative_paths\n",
    "    \n",
    "    return hard_anchor_paths, hard_negative_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa94d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_data_pipeline(anchors, comparisons, labels, batch_size=64, \n",
    "                                 prefetch_buffer=tf.data.AUTOTUNE, num_parallel_calls=tf.data.AUTOTUNE):\n",
    "    \"\"\"Create highly optimized data pipeline for maximum GPU utilization\"\"\"\n",
    "    \n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(comparisons)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "    dataset = dataset.shuffle(buffer_size=min(10000, len(anchors)))\n",
    "    dataset = dataset.map(\n",
    "        lambda a, c, l: (\n",
    "            (preprocess_with_augmentation(a, is_training=True),\n",
    "             preprocess_with_augmentation(c, is_training=True)),\n",
    "            tf.cast(l, tf.float32)\n",
    "        ),\n",
    "        num_parallel_calls=num_parallel_calls\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "    if len(anchors) < 50000:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d22443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_data(test_directory, num_pairs=1000):\n",
    "    \"\"\"Create validation data for verification accuracy monitoring\"\"\"\n",
    "    \n",
    "    person_dirs = [d for d in os.listdir(test_directory) \n",
    "                   if os.path.isdir(os.path.join(test_directory, d))]\n",
    "    \n",
    "    anchors, comparisons, labels = [], [], []\n",
    "    \n",
    "    for person in person_dirs[:50]:  # Limit for efficiency\n",
    "        person_path = os.path.join(test_directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:10]\n",
    "        \n",
    "        if len(images) >= 2:\n",
    "            for i in range(min(5, len(images)-1)):\n",
    "                anchors.append(images[i])\n",
    "                comparisons.append(images[i+1])\n",
    "                labels.append(1)\n",
    "\n",
    "    for i in range(len(anchors)):\n",
    "        if len(person_dirs) > 1:\n",
    "            # Select different person\n",
    "            other_person = random.choice([p for p in person_dirs[:50] if p != person_dirs[i % len(person_dirs)]])\n",
    "            other_person_path = os.path.join(test_directory, other_person)\n",
    "            other_images = [os.path.join(other_person_path, f) for f in os.listdir(other_person_path) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            if other_images:\n",
    "                anchors.append(anchors[i])  # Same anchor\n",
    "                comparisons.append(random.choice(other_images))\n",
    "                labels.append(0)\n",
    "    \n",
    "    return anchors[:num_pairs], comparisons[:num_pairs], labels[:num_pairs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7146bc",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d177c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, in_embed, valid_embed):\n",
    "        return tf.math.abs(in_embed - valid_embed)\n",
    "\n",
    "def create_optimized_embedding():\n",
    "    inputs = tf.keras.Input(shape=(100, 100, 3), name=\"input_img\")\n",
    "\n",
    "    x = tf.keras.layers.SeparableConv2D(64, (10, 10), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(256, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(512, activation='sigmoid', dtype='float32')(x) \n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='optimized_embedding')\n",
    "\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred is the distance between embeddings\n",
    "        # y_true is 1 for same person, 0 for different\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # For same person pairs, minimize distance\n",
    "        # For different person pairs, maximize distance up to margin\n",
    "        loss = y_true * tf.square(y_pred) + \\\n",
    "               (1 - y_true) * tf.square(tf.maximum(0.0, self.margin - y_pred))\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de285b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_embedding_with_attention():\n",
    "    inputs = tf.keras.Input(shape=(100, 100, 3), name=\"input_img\")  # Larger input size\n",
    "    \n",
    "    # Feature extraction backbone\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    attention = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "    x = tf.keras.layers.Multiply()([x, attention])\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    embeddings = tf.keras.layers.Dense(512, activation=None, dtype='float32')(x)\n",
    "    embeddings = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(embeddings)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=embeddings, name='advanced_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03fcb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred should be [anchor, positive, negative] embeddings\n",
    "        anchor, positive, negative = tf.split(y_pred, 3, axis=1)\n",
    "        \n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        loss = tf.maximum(0.0, pos_dist - neg_dist + self.margin)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32) \n",
    "        loss = y_true * tf.square(y_pred) + \\\n",
    "               (1 - y_true) * tf.square(tf.maximum(0.0, self.margin - y_pred))\n",
    "        return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1669e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistance(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        anchor, comparison = inputs\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(anchor - comparison), axis=-1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e72467db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_siamese_model():\n",
    "    embedding_network = create_advanced_embedding_with_attention()\n",
    "    anchor_input = Input(shape=(100, 100, 3), name='anchor')\n",
    "    comparison_input = Input(shape=(100, 100, 3), name='comparison')\n",
    "    anchor_embedding = embedding_network(anchor_input)\n",
    "    comparison_embedding = embedding_network(comparison_input)\n",
    "    distance = EuclideanDistance(name='euclidean_distance')([anchor_embedding, comparison_embedding])\n",
    "    prediction = Dense(1, activation='sigmoid', name='prediction', dtype='float32')(distance)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[anchor_input, comparison_input], \n",
    "                  outputs=[distance, prediction],  \n",
    "                  name='advanced_siamese')\n",
    "    \n",
    "    return model, embedding_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdc0e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_loss_model():\n",
    "    siamese_model, embedding_model = create_advanced_siamese_model()\n",
    "    \n",
    "    siamese_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\n",
    "            'euclidean_distance': ContrastiveLoss(margin=1.0),\n",
    "            'dense': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={'euclidean_distance': 1.0, 'prediction': 0.5},\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return siamese_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a4fa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cast(Layer):\n",
    "    def __init__(self, dtype=tf.float32, **kwargs):\n",
    "        super(Cast, self).__init__(**kwargs)\n",
    "        self.dtype_to_cast = dtype\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(inputs, self.dtype_to_cast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c50a03",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2706663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_enhance_existing_model(model_path='face_verification_v2.h5'):\n",
    "    \"\"\"Load existing model and prepare for enhanced training\"\"\"\n",
    "    print(f\"Loading existing model from {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Comprehensive custom objects dictionary\n",
    "        custom_objects = {\n",
    "            'L1Dist': L1Dist,\n",
    "            'EuclideanDistance': EuclideanDistance,\n",
    "            'ContrastiveLoss': ContrastiveLoss,\n",
    "            'Cast': Cast,  # Add Cast layer\n",
    "            'BinaryCrossentropy': tf.losses.BinaryCrossentropy,\n",
    "            'cast': tf.cast,  # Add cast function\n",
    "            'l2_normalize': tf.nn.l2_normalize,\n",
    "        }\n",
    "        \n",
    "        # Try loading with custom objects\n",
    "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "            model = load_model(model_path, compile=False)\n",
    "            \n",
    "        print(\"âœ… Model loaded successfully!\")\n",
    "        print(f\"Model outputs: {[output.name for output in model.outputs]}\")\n",
    "        \n",
    "        # Recompile the model\n",
    "        if len(model.outputs) > 1:\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                loss={\n",
    "                    'euclidean_distance': ContrastiveLoss(margin=1.0),\n",
    "                    'prediction': 'binary_crossentropy'\n",
    "                },\n",
    "                loss_weights={'euclidean_distance': 1.0, 'prediction': 0.5},\n",
    "                metrics={'prediction': 'accuracy'},\n",
    "                run_eagerly=False\n",
    "            )\n",
    "        else:\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                run_eagerly=False\n",
    "            )\n",
    "        \n",
    "        print(\"âœ… Model recompiled successfully!\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load model: {e}\")\n",
    "        print(\"Creating new model...\")\n",
    "        \n",
    "        model, _ = create_multi_loss_model()\n",
    "        print(\"âœ… New model created and compiled successfully!\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eac8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_easy_model(model, anchors, comparisons, labels, epochs=10, batch_size=32):\n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(comparisons)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "    dataset = dataset.map(lambda a, c, l: (\n",
    "        preprocess_with_augmentation(a, is_training=True),\n",
    "        preprocess_with_augmentation(c, is_training=True),\n",
    "        l\n",
    "    ), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    model.fit(\n",
    "        dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=3),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d42fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_training(model, train_directory, test_directory, total_epochs=50):\n",
    "    print(\"Stage 1: Easy examples (clear differences)\")\n",
    "    easy_anchors, easy_comparisons, easy_labels = create_optimized_pairs_from_directory(\n",
    "        train_directory, max_people=500, max_pairs_per_person=20\n",
    "    )\n",
    "    \n",
    "    train_easy_model(model, easy_anchors, easy_comparisons, easy_labels, epochs=15)\n",
    "    \n",
    "    print(\"Stage 2: Adding medium difficulty examples\")\n",
    "    medium_anchors, medium_comparisons, medium_labels = create_optimized_pairs_from_directory(\n",
    "        train_directory, max_people=1000, max_pairs_per_person=30\n",
    "    )\n",
    "    \n",
    "    all_anchors = easy_anchors + medium_anchors\n",
    "    all_comparisons = easy_comparisons + medium_comparisons\n",
    "    all_labels = easy_labels + medium_labels\n",
    "    \n",
    "    train_easy_model(model, all_anchors, all_comparisons, all_labels, epochs=20)\n",
    "    \n",
    "    print(\"Stage 3: Adding hard negatives\")\n",
    "    embedding_model = model.layers[2] \n",
    "    hard_anchors, hard_negatives = create_hard_negative_pairs(embedding_model, train_directory)\n",
    "    \n",
    "    all_anchors.extend(hard_anchors)\n",
    "    all_comparisons.extend(hard_negatives)\n",
    "    all_labels.extend([0] * len(hard_anchors))\n",
    "    \n",
    "    train_easy_model(model, all_anchors, all_comparisons, all_labels, epochs=15)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bb3077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_100_percent_accuracy():\n",
    "    siamese_model, embedding_model = create_multi_loss_model()\n",
    "    \n",
    "    trained_model = curriculum_training(\n",
    "        siamese_model, \n",
    "        TRAIN, \n",
    "        TEST,\n",
    "        total_epochs=50\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning on hard examples...\")\n",
    "    hard_anchors, hard_negatives = create_hard_negative_pairs(\n",
    "        embedding_model, \n",
    "        'data/training'\n",
    "    )\n",
    "    \n",
    "\n",
    "    trained_model.save('face_verification_advanced.h5')\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6fe1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_curriculum_training(model, train_directory, test_directory, \n",
    "                               initial_epochs=20, fine_tune_epochs=30, batch_size=64):\n",
    "    \"\"\"Advanced curriculum training with GPU optimization and early stopping\"\"\"\n",
    "    \n",
    "    print(\"Creating validation data for monitoring...\")\n",
    "    val_anchors, val_comparisons, val_labels = create_validation_data(test_directory)\n",
    "    \n",
    "    print(\"\\n=== Stage 1: Enhanced Training on Diverse Data ===\")\n",
    "\n",
    "    train_anchors, train_comparisons, train_labels = create_optimized_pairs_from_directory(\n",
    "        train_directory, max_people=2000, max_pairs_per_person=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(train_anchors)} training pairs\")\n",
    "\n",
    "    train_dataset = create_optimized_data_pipeline(\n",
    "        train_anchors, train_comparisons, train_labels, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        VerificationAccuracyEarlyStopping(\n",
    "            validation_data=(val_anchors, val_comparisons, val_labels),\n",
    "            patience=7,\n",
    "            min_verification_acc=0.70,\n",
    "            verbose=1\n",
    "        ),\n",
    "        GPUUtilizationMonitor(),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'face_verification_enhanced.h5',\n",
    "            monitor='loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    print(\"Starting enhanced training...\")\n",
    "    history1 = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=initial_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Stage 2: Fine-tuning on Hard Examples ===\")\n",
    "    embedding_model = None\n",
    "    for layer in model.layers:\n",
    "        if 'embedding' in layer.name.lower():\n",
    "            embedding_model = layer\n",
    "            break\n",
    "    \n",
    "    if embedding_model is None:\n",
    "        embedding_model = Model(inputs=model.input[0], outputs=model.layers[-3].output)\n",
    "    \n",
    "    hard_anchors, hard_negatives = create_hard_negative_pairs(\n",
    "        embedding_model, train_directory, num_hard_negatives=2000\n",
    "    )\n",
    "\n",
    "    all_anchors = train_anchors + hard_anchors\n",
    "    all_comparisons = train_comparisons + hard_negatives\n",
    "    all_labels = train_labels + [0] * len(hard_anchors)\n",
    "    \n",
    "    print(f\"Added {len(hard_anchors)} hard negative pairs\")\n",
    "\n",
    "    fine_tune_dataset = create_optimized_data_pipeline(\n",
    "        all_anchors, all_comparisons, all_labels, batch_size=batch_size//2  \n",
    "    )\n",
    "    \n",
    "    model.optimizer.learning_rate = 5e-6\n",
    "    callbacks[0].min_verification_acc = 0.85  \n",
    "    callbacks[0].patience = 5\n",
    "    \n",
    "    print(\"Starting fine-tuning...\")\n",
    "    history2 = model.fit(\n",
    "        fine_tune_dataset,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Stage 3: Final Optimization ===\")\n",
    "    \n",
    "    model.optimizer.learning_rate = 1e-6\n",
    "    \n",
    "    final_dataset = create_optimized_data_pipeline(\n",
    "        all_anchors, all_comparisons, all_labels, batch_size=32\n",
    "    )\n",
    "\n",
    "    callbacks[0].min_verification_acc = 0.95 \n",
    "    callbacks[0].patience = 3\n",
    "    \n",
    "    print(\"Starting final optimization...\")\n",
    "    history3 = model.fit(\n",
    "        final_dataset,\n",
    "        epochs=20,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, [history1, history2, history3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caa64aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_performance(model, test_directory):   \n",
    "    print(\"\\n=== Final Performance Evaluation ===\")\n",
    "    \n",
    "    test_anchors, test_comparisons, test_labels = create_validation_data(test_directory, num_pairs=2000)\n",
    "    predictions = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in range(0, len(test_anchors), batch_size):\n",
    "        batch_anchors = test_anchors[i:i+batch_size]\n",
    "        batch_comparisons = test_comparisons[i:i+batch_size]\n",
    "        \n",
    "        processed_anchors = tf.stack([preprocess_with_augmentation(img, is_training=False) \n",
    "                                    for img in batch_anchors])\n",
    "        processed_comparisons = tf.stack([preprocess_with_augmentation(img, is_training=False) \n",
    "                                        for img in batch_comparisons])\n",
    "        \n",
    "        _, batch_preds = model([processed_anchors, processed_comparisons])\n",
    "        predictions.extend(batch_preds.numpy().flatten())\n",
    "    \n",
    "    binary_preds = (np.array(predictions) > 0.5).astype(int)\n",
    "    final_accuracy = accuracy_score(test_labels, binary_preds)\n",
    "    \n",
    "    print(f\"Final Verification Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    precision = precision_score(test_labels, binary_preds)\n",
    "    recall = recall_score(test_labels, binary_preds)\n",
    "    f1 = f1_score(test_labels, binary_preds)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return final_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a01cccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_perfect_verification():\n",
    "    \n",
    "    print(\"Loading existing model and enhancing it...\")\n",
    "    model = load_and_enhance_existing_model('face_verification_v2.h5')\n",
    "    \n",
    "    print(\"Starting advanced curriculum training...\")\n",
    "    enhanced_model, training_histories = advanced_curriculum_training(\n",
    "        model, TRAIN, TEST, \n",
    "        initial_epochs=25, \n",
    "        fine_tune_epochs=35, \n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluating final performance...\")\n",
    "    final_accuracy, precision, recall, f1 = evaluate_final_performance(enhanced_model, TEST)\n",
    "\n",
    "    enhanced_model.save('face_verification_perfect.h5')\n",
    "    print(\"Enhanced model saved as 'face_verification_perfect.h5'\")\n",
    "    \n",
    "    return enhanced_model, final_accuracy, training_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f91b50",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bfe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model and enhancing it...\n",
      "Loading existing model from face_verification_v2.h5\n",
      "WARNING:tensorflow:From c:\\Users\\lokna\\Projects\\MyReactNativeApp\\extensions\\face_auth\\tensorflow_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "âœ… Model loaded successfully!\n",
      "Model outputs: ['keras_tensor_43']\n",
      "âœ… Model recompiled successfully!\n",
      "Starting advanced curriculum training...\n",
      "Creating validation data for monitoring...\n",
      "\n",
      "=== Stage 1: Enhanced Training on Diverse Data ===\n",
      "Using 2000 people (limited from potentially more)\n",
      "Created 5751 positive pairs\n",
      "Created 5751 negative pairs\n",
      "âœ… Final dataset: 5751 positive, 5751 negative pairs\n",
      "âœ… Total pairs: 11502\n",
      "Created 11502 training pairs\n",
      "Starting enhanced training...\n",
      "Epoch 1/25\n",
      "\u001b[1m 16/179\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:36:18\u001b[0m 58s/step - accuracy: 0.5023 - loss: 0.8441"
     ]
    }
   ],
   "source": [
    "final_model, accuracy, histories = train_to_perfect_verification()\n",
    "\n",
    "print(f\"\\nTraining Complete!\")\n",
    "print(f\"Final Verification Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "if accuracy >= 0.95:\n",
    "    print(\"ðŸŽ‰ Achieved near-perfect verification accuracy!\")\n",
    "else:\n",
    "    print(f\"Current accuracy: {accuracy*100:.2f}%. Consider additional training cycles.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
