{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ebd1e7",
   "metadata": {},
   "source": [
    "# Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52deb349",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ea27476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re\n",
    "import uuid\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import shutil\n",
    "from tensorflow.keras.mixed_precision import set_global_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1aa2a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model(inputs = [inputImg, veriImg], outputs = [1,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a524d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6633a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.enable_tensor_float_32()\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12e485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = os.path.join('data', 'training')\n",
    "TEST = os.path.join('data', 'test')\n",
    "ARCH = os.path.join('data', 'archive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df77a32",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6831fe7",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a53fe7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    for d in [TRAIN, TEST]:\n",
    "        Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_folders = [\n",
    "        folder for folder in os.listdir(ARCH)\n",
    "        if os.path.isdir(os.path.join(ARCH, folder))\n",
    "    ]\n",
    "\n",
    "    random.shuffle(all_folders)\n",
    "    split_idx = int(len(all_folders) * 0.7)\n",
    "    train_folders = all_folders[:split_idx]\n",
    "    test_folders = all_folders[split_idx:]\n",
    "\n",
    "    for folder_name in train_folders:\n",
    "        src = os.path.join(ARCH, folder_name)\n",
    "        dest = os.path.join(TRAIN, folder_name)\n",
    "        shutil.copytree(src, dest)\n",
    "        print(f\"Copied to training: {folder_name}\")\n",
    "\n",
    "    for folder_name in test_folders:\n",
    "        src = os.path.join(ARCH, folder_name)\n",
    "        dest = os.path.join(TEST, folder_name)\n",
    "        shutil.copytree(src, dest)\n",
    "        print(f\"Copied to testing: {folder_name}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. {len(train_folders)} folders in training, {len(test_folders)} in test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37c0e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting function because I messed up the top one and did it twice once with folder deleting and once without lol\n",
    "def remove_duplicate_images(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Error: Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    files_to_delete = []\n",
    "    pattern = re.compile(r'^(.+)_1(\\.[^.]+)$')\n",
    "    deleted_count = 0\n",
    "    \n",
    "    print(f\"Scanning directory: {directory}\")\n",
    "\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            extension = match.group(2)\n",
    "            original_file = f\"{base_name}{extension}\"\n",
    "            if original_file in files:\n",
    "                files_to_delete.append(file)\n",
    "\n",
    "    for file in files_to_delete:\n",
    "        try:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nTotal duplicate files deleted: {deleted_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60d5eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = frame[120:120+250, 200:200+250, :]\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('a'):\n",
    "        img_name = os.path.join(ANC, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(f\"Saved anchor image: {img_name}\")\n",
    "    elif key == ord('p'):\n",
    "        img_name = os.path.join(POS, f'{uuid.uuid1()}.jpg')\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(f\"Saved positive image: {img_name}\")\n",
    "    elif key == ord('q'):\n",
    "        print(\"Quitting...\")\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea202e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_from_directory(directory):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    print(f\"Found {len(person_dirs)} people in '{directory}'\")\n",
    "\n",
    "    anchor_paths = []\n",
    "    positive_paths = []\n",
    "    negative_paths = []\n",
    "\n",
    "    total_skipped = 0\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "\n",
    "        for i in range(len(person_images)):\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                anchor_paths.append(person_images[i])\n",
    "                positive_paths.append(person_images[j])\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        for anchor_img in person_images[:10]:\n",
    "            sampled_others = random.sample(other_people, min(len(other_people), 10))  \n",
    "            for other_person in sampled_others:\n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(5, len(other_person_images))):\n",
    "                    negative_img = random.choice(other_person_images)\n",
    "                    anchor_paths.append(anchor_img)\n",
    "                    negative_paths.append(negative_img)\n",
    "\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(person_dirs):\n",
    "            print(f\"[{idx + 1}/{len(person_dirs)}] Processed '{person}': \"\n",
    "                  f\"{len(person_images)} imgs, total positives: {len(positive_paths)}, \"\n",
    "                  f\"negatives: {len(negative_paths)}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Finished pair creation.\")\n",
    "    print(f\"  - Skipped people with <2 images: {total_skipped}\")\n",
    "    print(f\"  - Total positive pairs: {len(positive_paths)}\")\n",
    "    print(f\"  - Total negative pairs: {len(negative_paths)}\")\n",
    "\n",
    "    positive_labels = tf.ones(len(positive_paths))\n",
    "    negative_labels = tf.zeros(len(negative_paths))\n",
    "\n",
    "    all_anchor_paths = anchor_paths + anchor_paths\n",
    "    all_comparison_paths = positive_paths + negative_paths\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    print(f\"  - Final dataset size: {len(all_labels)} pairs (anchors: {len(all_anchor_paths)})\")\n",
    "\n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(all_anchor_paths)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(all_comparison_paths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(all_labels)\n",
    "\n",
    "    return anchor_ds, comparison_ds, labels_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b807c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_pairs_from_directory(directory, max_people=1500, max_pairs_per_person=200):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "\n",
    "    if len(person_dirs) > max_people:\n",
    "        person_dirs = np.random.choice(person_dirs, max_people, replace=False)\n",
    "    \n",
    "    print(f\"Using {len(person_dirs)} people (limited from potentially more)\")\n",
    "    pos_anchor_paths = []\n",
    "    pos_comparison_paths = []\n",
    "    neg_anchor_paths = []\n",
    "    neg_comparison_paths = []\n",
    "\n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 2:\n",
    "            continue\n",
    "\n",
    "        pair_count = 0\n",
    "        for i in range(len(person_images)):\n",
    "            if pair_count >= max_pairs_per_person:\n",
    "                break\n",
    "            for j in range(i+1, len(person_images)):\n",
    "                if pair_count >= max_pairs_per_person:\n",
    "                    break\n",
    "                pos_anchor_paths.append(person_images[i])\n",
    "                pos_comparison_paths.append(person_images[j])\n",
    "                pair_count += 1\n",
    "\n",
    "    print(f\"Created {len(pos_anchor_paths)} positive pairs\")\n",
    "\n",
    "    target_negative_pairs = len(pos_anchor_paths)\n",
    "    \n",
    "    for idx, person in enumerate(person_dirs):\n",
    "        if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "            break\n",
    "            \n",
    "        person_path = os.path.join(directory, person)\n",
    "        person_images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        if len(person_images) < 1:\n",
    "            continue\n",
    "\n",
    "        other_people = [p for p in person_dirs if p != person]\n",
    "        if not other_people:\n",
    "            continue\n",
    "\n",
    "        for anchor_img in person_images: \n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "\n",
    "            sampled_others = np.random.choice(other_people, min(len(other_people), 20), replace=False)\n",
    "            \n",
    "            for other_person in sampled_others:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                other_person_path = os.path.join(directory, other_person)\n",
    "                other_person_images = [os.path.join(other_person_path, f) \n",
    "                                       for f in os.listdir(other_person_path) \n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "                if not other_person_images:\n",
    "                    continue\n",
    "\n",
    "                for _ in range(min(8, len(other_person_images))):  \n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                    negative_img = np.random.choice(other_person_images)\n",
    "                    neg_anchor_paths.append(anchor_img)\n",
    "                    neg_comparison_paths.append(negative_img)\n",
    "\n",
    "    print(f\"Created {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"‚úÖ Final dataset: {len(pos_anchor_paths)} positive, {len(neg_anchor_paths)} negative pairs\")\n",
    "    print(f\"‚úÖ Total pairs: {len(pos_anchor_paths) + len(neg_anchor_paths)}\")\n",
    "    \n",
    "    all_anchor_paths = pos_anchor_paths + neg_anchor_paths\n",
    "    all_comparison_paths = pos_comparison_paths + neg_comparison_paths\n",
    "\n",
    "    positive_labels = tf.ones(len(pos_anchor_paths))\n",
    "    negative_labels = tf.zeros(len(neg_anchor_paths))\n",
    "    all_labels = tf.concat([positive_labels, negative_labels], axis=0)\n",
    "\n",
    "    return all_anchor_paths, all_comparison_paths, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1688e1",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "595c8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_path):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img,(100,100))\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cac5cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_preprocess(img_path):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    img = tf.image.resize(img, (100, 100), method='bilinear')\n",
    "    img = tf.cast(img, tf.float16) / 255.0 \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7adda82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_twin(in_img, valid_img, label):\n",
    "    return(preprocess(in_img), preprocess(valid_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3d7373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_preproc_twin(anchor_path, comparison_path, label):\n",
    "    return (\n",
    "        optimized_preprocess(anchor_path), \n",
    "        optimized_preprocess(comparison_path), \n",
    "        tf.cast(label, tf.float16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc21ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    anchor_ds, comparison_ds, labels_ds = create_pairs_from_directory(TRAIN)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "    dataset = dataset.map(preproc_twin)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(buffer_size=1024)\n",
    "    \n",
    "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(16)\n",
    "    train_dataset = train_dataset.prefetch(8)\n",
    "    \n",
    "    val_dataset = val_dataset.batch(16)\n",
    "    val_dataset = val_dataset.prefetch(8)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9e2037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimized_datasets(train_dir, batch_size=32, prefetch_size=tf.data.AUTOTUNE):\n",
    "    anchor_paths, comparison_paths, labels = create_optimized_pairs_from_directory(\n",
    "        train_dir, max_people=1500, max_pairs_per_person=30\n",
    "    )\n",
    "    \n",
    "    anchor_ds = tf.data.Dataset.from_tensor_slices(anchor_paths)\n",
    "    comparison_ds = tf.data.Dataset.from_tensor_slices(comparison_paths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        optimized_preproc_twin, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,  \n",
    "        deterministic=False  \n",
    "    )\n",
    "\n",
    "    dataset = dataset.cache()  \n",
    "    dataset = dataset.shuffle(buffer_size=min(10000, len(anchor_paths)))\n",
    "    dataset_size = len(anchor_paths)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    \n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9d7e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_dataset_generator(directory, max_people=1500, max_pairs_per_person=50):\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    person_images = {}\n",
    "    \n",
    "    # Pre-load all image paths\n",
    "    for person in person_dirs:\n",
    "        person_path = os.path.join(directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if len(images) >= 2:\n",
    "            person_images[person] = images\n",
    "    \n",
    "    people_list = list(person_images.keys())\n",
    "    if len(people_list) > max_people:\n",
    "        people_list = np.random.choice(people_list, max_people, replace=False).tolist()\n",
    "        \n",
    "    print(f\"Loaded {len(people_list)} people for dynamic generation\")\n",
    "    \n",
    "    def generate_epoch_dataset(epoch_num):\n",
    "        # Set different random seed for each epoch\n",
    "        np.random.seed(42 + epoch_num)\n",
    "        random.seed(42 + epoch_num)\n",
    "        \n",
    "        pos_anchor_paths = []\n",
    "        pos_comparison_paths = []\n",
    "        neg_anchor_paths = []\n",
    "        neg_comparison_paths = []\n",
    "        \n",
    "        # Generate positive pairs\n",
    "        print(f\"Generating positive pairs for epoch {epoch_num}...\")\n",
    "        for person in people_list:\n",
    "            images = person_images[person]\n",
    "            \n",
    "            # Create positive pairs - sample different combinations each epoch\n",
    "            num_pairs = min(max_pairs_per_person, len(images) * (len(images) - 1) // 2)\n",
    "            \n",
    "            pairs_created = 0\n",
    "            attempts = 0\n",
    "            max_attempts = num_pairs * 3  # Prevent infinite loops\n",
    "            \n",
    "            while pairs_created < num_pairs and attempts < max_attempts:\n",
    "                i = random.randint(0, len(images) - 1)\n",
    "                j = random.randint(0, len(images) - 1)\n",
    "                if i != j:  # Different images\n",
    "                    pos_anchor_paths.append(images[i])\n",
    "                    pos_comparison_paths.append(images[j])\n",
    "                    pairs_created += 1\n",
    "                attempts += 1\n",
    "        \n",
    "        # Generate negative pairs\n",
    "        print(f\"Generating negative pairs for epoch {epoch_num}...\")\n",
    "        target_negative_pairs = len(pos_anchor_paths)  # Match positive pairs\n",
    "        \n",
    "        for person in people_list:\n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "                \n",
    "            images = person_images[person]\n",
    "            other_people = [p for p in people_list if p != person]\n",
    "            \n",
    "            if not other_people:\n",
    "                continue\n",
    "                \n",
    "            # Sample anchor images for this person\n",
    "            anchor_samples = min(10, len(images))\n",
    "            selected_anchors = random.sample(images, anchor_samples)\n",
    "            \n",
    "            for anchor_img in selected_anchors:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Pick random other people for negatives\n",
    "                num_others = min(10, len(other_people))\n",
    "                selected_others = random.sample(other_people, num_others)\n",
    "                \n",
    "                for other_person in selected_others:\n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                        \n",
    "                    other_images = person_images[other_person]\n",
    "                    if other_images:\n",
    "                        negative_img = random.choice(other_images)\n",
    "                        neg_anchor_paths.append(anchor_img)\n",
    "                        neg_comparison_paths.append(negative_img)\n",
    "        \n",
    "        # Balance the dataset\n",
    "        min_size = min(len(pos_anchor_paths), len(neg_anchor_paths))\n",
    "        \n",
    "        # Randomly sample to balance\n",
    "        pos_indices = random.sample(range(len(pos_anchor_paths)), min_size)\n",
    "        neg_indices = random.sample(range(len(neg_anchor_paths)), min_size)\n",
    "        \n",
    "        final_anchors = [pos_anchor_paths[i] for i in pos_indices] + [neg_anchor_paths[i] for i in neg_indices]\n",
    "        final_comparisons = [pos_comparison_paths[i] for i in pos_indices] + [neg_comparison_paths[i] for i in neg_indices]\n",
    "        final_labels = [1.0] * min_size + [0.0] * min_size\n",
    "        \n",
    "        # Shuffle everything together\n",
    "        combined = list(zip(final_anchors, final_comparisons, final_labels))\n",
    "        random.shuffle(combined)\n",
    "        final_anchors, final_comparisons, final_labels = zip(*combined)\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}: Generated {len(final_labels)} pairs ({min_size} pos, {min_size} neg)\")\n",
    "        \n",
    "        return list(final_anchors), list(final_comparisons), list(final_labels)\n",
    "    \n",
    "    return generate_epoch_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c2145",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65e9f2",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7090c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeding_make():\n",
    "    in_ = Input(shape=(100,100,3), name=\"in img\")\n",
    "\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(in_)\n",
    "    p1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(p1)\n",
    "    p2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(p2)\n",
    "    p3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(p3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096,activation='sigmoid')(f1)\n",
    "\n",
    "    return Model(inputs=in_, outputs=d1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d1da1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_embedding():\n",
    "    inputs = tf.keras.Input(shape=(100, 100, 3), name=\"input_img\")\n",
    "\n",
    "    x = tf.keras.layers.SeparableConv2D(64, (10, 10), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(128, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.SeparableConv2D(256, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(512, activation='sigmoid', dtype='float32')(x) \n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='optimized_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "862d3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, in_embed, valid_embed):\n",
    "        return tf.math.abs(in_embed - valid_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7e57168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(): #Simanese\n",
    "    input_img = Input(name='input_img', shape=(100,100,3))\n",
    "    validation_img = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    model_layer = L1Dist()\n",
    "    model_layer.name = 'distance'\n",
    "    distances = model_layer(embedding(input_img), embedding(validation_img))\n",
    "\n",
    "    classifier = Dense(1,activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_img, validation_img], outputs=classifier, name='SimaneseNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78180c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96043ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "188fb627",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_optimized_embedding()\n",
    "siamese_model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "20f69853",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoints, 'ckpt2')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc42fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def t_step(batch, model, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:  \n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        y_pred = model([anchor_img, comparison_img], training=True)\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8fee9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimized_train_step(batch, model, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        y_pred = model([anchor_img, comparison_img], training=True)\n",
    "        \n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss) if hasattr(optimizer, 'get_scaled_loss') else loss\n",
    "\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    if hasattr(optimizer, 'get_unscaled_gradients'):\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    else:\n",
    "        gradients = scaled_gradients\n",
    "    gradients = [tf.clip_by_norm(grad, 1.0) if grad is not None else grad for grad in gradients]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e7a307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, epochs))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))  \n",
    "        for idx, batch in enumerate(data): \n",
    "            t_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e9656e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, epochs=50):\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        \n",
    "        progress_bar = tf.keras.utils.Progbar(len(train_dataset))\n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            loss = t_step(batch, model, opt, binary_cross_loss)\n",
    "            \n",
    "            train_loss(loss)\n",
    "            train_accuracy(batch[2], model([batch[0], batch[1]], training=False))\n",
    "            progress_bar.update(batch_idx + 1)\n",
    "\n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(batch[2], val_preds)\n",
    "\n",
    "        print(f\"Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result():.4f}, Val Accuracy: {val_accuracy.result():.4f}\")\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dfdcb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_retrain_model(model, train_dataset, val_dataset, epochs=10):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    if hasattr(tf.keras.mixed_precision, 'LossScaleOptimizer'):\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "   \n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "    best_val_acc = 0\n",
    "    patience = 3\n",
    "    wait = 0\n",
    "    best_weights_saved = False\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        num_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "        progress_bar = tf.keras.utils.Progbar(num_batches)\n",
    "       \n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            loss = optimized_train_step(batch, model, optimizer, loss_fn)\n",
    "            train_loss(loss)\n",
    "            preds = model([batch[0], batch[1]], training=False)\n",
    "            train_accuracy(tf.cast(batch[2], tf.float32), tf.cast(preds, tf.float32))\n",
    "           \n",
    "            progress_bar.update(batch_idx + 1)\n",
    "       \n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(tf.cast(batch[2], tf.float32), tf.cast(val_preds, tf.float32))\n",
    "       \n",
    "        current_val_acc = val_accuracy.result()\n",
    "        print(f\"Loss: {train_loss.result():.4f}, \"\n",
    "              f\"Accuracy: {train_accuracy.result():.4f}, \"\n",
    "              f\"Val Accuracy: {current_val_acc:.4f}\")\n",
    "        \n",
    "        if current_val_acc > best_val_acc:\n",
    "            best_val_acc = current_val_acc\n",
    "            wait = 0\n",
    "            model.save_weights('best_model.weights.h5')\n",
    "            best_weights_saved = True\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.4f} - weights saved\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_weights_saved:\n",
    "        model.load_weights('best_model.weights.h5')\n",
    "        print(\"Loaded best weights\")\n",
    "    else:\n",
    "        print(\"No improvement found, keeping current weights\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f33407c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dynamic_dataset_generator(directory, max_people=1500, max_pairs_per_person=50):\n",
    "    \"\"\"\n",
    "    Creates a generator that produces different dataset each epoch\n",
    "    \"\"\"\n",
    "    person_dirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    person_images = {}\n",
    "    \n",
    "    # Pre-load all image paths\n",
    "    for person in person_dirs:\n",
    "        person_path = os.path.join(directory, person)\n",
    "        images = [os.path.join(person_path, f) for f in os.listdir(person_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if len(images) >= 2:\n",
    "            person_images[person] = images\n",
    "    \n",
    "    people_list = list(person_images.keys())\n",
    "    if len(people_list) > max_people:\n",
    "        people_list = np.random.choice(people_list, max_people, replace=False).tolist()\n",
    "        \n",
    "    print(f\"Loaded {len(people_list)} people for dynamic generation\")\n",
    "    \n",
    "    def generate_epoch_dataset(epoch_num):\n",
    "        # Set different random seed for each epoch\n",
    "        np.random.seed(42 + epoch_num)\n",
    "        random.seed(42 + epoch_num)\n",
    "        \n",
    "        pos_anchor_paths = []\n",
    "        pos_comparison_paths = []\n",
    "        neg_anchor_paths = []\n",
    "        neg_comparison_paths = []\n",
    "        \n",
    "        # Generate positive pairs\n",
    "        print(f\"Generating positive pairs for epoch {epoch_num}...\")\n",
    "        for person in people_list:\n",
    "            images = person_images[person]\n",
    "            \n",
    "            # Create positive pairs - sample different combinations each epoch\n",
    "            num_pairs = min(max_pairs_per_person, len(images) * (len(images) - 1) // 2)\n",
    "            \n",
    "            pairs_created = 0\n",
    "            attempts = 0\n",
    "            max_attempts = num_pairs * 3  # Prevent infinite loops\n",
    "            \n",
    "            while pairs_created < num_pairs and attempts < max_attempts:\n",
    "                i = random.randint(0, len(images) - 1)\n",
    "                j = random.randint(0, len(images) - 1)\n",
    "                if i != j:  # Different images\n",
    "                    pos_anchor_paths.append(images[i])\n",
    "                    pos_comparison_paths.append(images[j])\n",
    "                    pairs_created += 1\n",
    "                attempts += 1\n",
    "        \n",
    "        # Generate negative pairs\n",
    "        print(f\"Generating negative pairs for epoch {epoch_num}...\")\n",
    "        target_negative_pairs = len(pos_anchor_paths)  # Match positive pairs\n",
    "        \n",
    "        for person in people_list:\n",
    "            if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                break\n",
    "                \n",
    "            images = person_images[person]\n",
    "            other_people = [p for p in people_list if p != person]\n",
    "            \n",
    "            if not other_people:\n",
    "                continue\n",
    "                \n",
    "            # Sample anchor images for this person\n",
    "            anchor_samples = min(10, len(images))\n",
    "            selected_anchors = random.sample(images, anchor_samples)\n",
    "            \n",
    "            for anchor_img in selected_anchors:\n",
    "                if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Pick random other people for negatives\n",
    "                num_others = min(10, len(other_people))\n",
    "                selected_others = random.sample(other_people, num_others)\n",
    "                \n",
    "                for other_person in selected_others:\n",
    "                    if len(neg_anchor_paths) >= target_negative_pairs:\n",
    "                        break\n",
    "                        \n",
    "                    other_images = person_images[other_person]\n",
    "                    if other_images:\n",
    "                        negative_img = random.choice(other_images)\n",
    "                        neg_anchor_paths.append(anchor_img)\n",
    "                        neg_comparison_paths.append(negative_img)\n",
    "        \n",
    "        # Balance the dataset\n",
    "        min_size = min(len(pos_anchor_paths), len(neg_anchor_paths))\n",
    "        \n",
    "        # Randomly sample to balance\n",
    "        pos_indices = random.sample(range(len(pos_anchor_paths)), min_size)\n",
    "        neg_indices = random.sample(range(len(neg_anchor_paths)), min_size)\n",
    "        \n",
    "        final_anchors = [pos_anchor_paths[i] for i in pos_indices] + [neg_anchor_paths[i] for i in neg_indices]\n",
    "        final_comparisons = [pos_comparison_paths[i] for i in pos_indices] + [neg_comparison_paths[i] for i in neg_indices]\n",
    "        final_labels = [1.0] * min_size + [0.0] * min_size\n",
    "        \n",
    "        # Shuffle everything together\n",
    "        combined = list(zip(final_anchors, final_comparisons, final_labels))\n",
    "        random.shuffle(combined)\n",
    "        final_anchors, final_comparisons, final_labels = zip(*combined)\n",
    "        \n",
    "        print(f\"Epoch {epoch_num}: Generated {len(final_labels)} pairs ({min_size} pos, {min_size} neg)\")\n",
    "        \n",
    "        return list(final_anchors), list(final_comparisons), list(final_labels)\n",
    "    \n",
    "    return generate_epoch_dataset\n",
    "\n",
    "def train_with_dynamic_datasets(model, train_directory, test_directory, epochs=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Training loop with dynamic dataset generation + fixed validation\n",
    "    \"\"\"\n",
    "    # Simple, robust training step function\n",
    "    @tf.function\n",
    "    def simple_train_step(batch, model, optimizer, loss_fn):\n",
    "        anchor_img, comparison_img, y_true = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model([anchor_img, comparison_img], training=True)\n",
    "            y_pred = tf.cast(y_pred, tf.float32)\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            loss = loss_fn(y_true, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        gradients = [tf.clip_by_norm(grad, 1.0) if grad is not None else grad for grad in gradients]\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Dynamic training data generator\n",
    "    train_generator = create_dynamic_dataset_generator(train_directory, max_people=1500, max_pairs_per_person=30)\n",
    "    \n",
    "    # Fixed validation dataset (created once, reused every epoch)\n",
    "    print(\"Creating fixed validation dataset...\")\n",
    "    val_anchors, val_comparisons, val_labels = create_optimized_pairs_from_directory(\n",
    "        test_directory, max_people=300, max_pairs_per_person=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation dataset: {len(val_labels)} pairs\")\n",
    "    \n",
    "    # Create validation TensorFlow dataset\n",
    "    val_anchor_ds = tf.data.Dataset.from_tensor_slices(val_anchors)\n",
    "    val_comparison_ds = tf.data.Dataset.from_tensor_slices(val_comparisons)\n",
    "    val_labels_ds = tf.data.Dataset.from_tensor_slices(val_labels)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.zip((val_anchor_ds, val_comparison_ds, val_labels_ds))\n",
    "    val_dataset = val_dataset.map(optimized_preproc_twin, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 5\n",
    "    wait = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "        \n",
    "        # Generate NEW training dataset for this epoch\n",
    "        anchors, comparisons, labels = train_generator(epoch)\n",
    "        print(f\"Training dataset: {len(labels)} pairs\")\n",
    "        \n",
    "        # Create TensorFlow dataset for this epoch\n",
    "        anchor_ds = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "        comparison_ds = tf.data.Dataset.from_tensor_slices(comparisons)\n",
    "        labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.zip((anchor_ds, comparison_ds, labels_ds))\n",
    "        train_dataset = train_dataset.map(optimized_preproc_twin, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        \n",
    "        num_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "        progress_bar = tf.keras.utils.Progbar(num_batches)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataset):\n",
    "            # Use the simple training step\n",
    "            loss = simple_train_step(batch, model, optimizer, loss_fn)\n",
    "            \n",
    "            # Ensure loss is not None and is a proper tensor\n",
    "            if loss is not None:\n",
    "                train_loss(loss)\n",
    "                \n",
    "                preds = model([batch[0], batch[1]], training=False)\n",
    "                train_accuracy(tf.cast(batch[2], tf.float32), tf.cast(preds, tf.float32))\n",
    "            else:\n",
    "                print(f\"Warning: Loss is None at batch {batch_idx}\")\n",
    "            \n",
    "            progress_bar.update(batch_idx + 1)\n",
    "        \n",
    "        # Validation phase (using FIXED validation set)\n",
    "        val_accuracy.reset_state()\n",
    "        for batch in val_dataset:\n",
    "            val_preds = model([batch[0], batch[1]], training=False)\n",
    "            val_accuracy(tf.cast(batch[2], tf.float32), tf.cast(val_preds, tf.float32))\n",
    "        \n",
    "        current_val_acc = val_accuracy.result()\n",
    "        print(f\"Loss: {train_loss.result():.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy.result():.4f}, \"\n",
    "              f\"Val Acc: {current_val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if current_val_acc > best_val_acc:\n",
    "            best_val_acc = current_val_acc\n",
    "            wait = 0\n",
    "            model.save_weights('best_dynamic_model.weights.h5')\n",
    "            print(f\"üéØ New best validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save_weights(f'dynamic_model_epoch_{epoch+1}.weights.h5')\n",
    "    \n",
    "    # Load best weights\n",
    "    if os.path.exists('best_dynamic_model.weights.h5'):\n",
    "        model.load_weights('best_dynamic_model.weights.h5')\n",
    "        print(f\"‚úÖ Training complete. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fb2ec2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "caa7ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(train_data, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab9fa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain_model(siamese_model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1b789ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1500 people (limited from potentially more)\n",
      "Created 3152 positive pairs\n",
      "Created 3152 negative pairs\n",
      "‚úÖ Final dataset: 3152 positive, 3152 negative pairs\n",
      "‚úÖ Total pairs: 6304\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = prepare_optimized_datasets(\n",
    "    TRAIN, \n",
    "    batch_size=64,\n",
    "    prefetch_size=tf.data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1748dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_model = optimized_retrain_model(\n",
    "#    siamese_model, \n",
    "#    train_dataset, \n",
    "#    val_dataset, \n",
    "#    epochs=20\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf102cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1159 people for dynamic generation\n",
      "Creating fixed validation dataset...\n",
      "Using 300 people (limited from potentially more)\n",
      "Created 771 positive pairs\n",
      "Created 771 negative pairs\n",
      "‚úÖ Final dataset: 771 positive, 771 negative pairs\n",
      "‚úÖ Total pairs: 1542\n",
      "Validation dataset: 1542 pairs\n",
      "\n",
      "=== Epoch 1/20 ===\n",
      "Generating positive pairs for epoch 0...\n",
      "Generating negative pairs for epoch 0...\n",
      "Epoch 0: Generated 17026 pairs (8513 pos, 8513 neg)\n",
      "Training dataset: 17026 pairs\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23354s\u001b[0m 44s/step\n",
      "Loss: 0.6918, Train Acc: 0.5256, Val Acc: 0.5189\n",
      "üéØ New best validation accuracy: 0.5189\n",
      "\n",
      "=== Epoch 2/20 ===\n",
      "Generating positive pairs for epoch 1...\n",
      "Generating negative pairs for epoch 1...\n",
      "Epoch 1: Generated 17038 pairs (8519 pos, 8519 neg)\n",
      "Training dataset: 17038 pairs\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21854s\u001b[0m 41s/step\n",
      "Loss: 0.6856, Train Acc: 0.5403, Val Acc: 0.5684\n",
      "üéØ New best validation accuracy: 0.5684\n",
      "\n",
      "=== Epoch 3/20 ===\n",
      "Generating positive pairs for epoch 2...\n",
      "Generating negative pairs for epoch 2...\n",
      "Epoch 2: Generated 17022 pairs (8511 pos, 8511 neg)\n",
      "Training dataset: 17022 pairs\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19554s\u001b[0m 37s/step\n",
      "Loss: 0.6579, Train Acc: 0.6041, Val Acc: 0.6191\n",
      "üéØ New best validation accuracy: 0.6191\n",
      "\n",
      "=== Epoch 4/20 ===\n",
      "Generating positive pairs for epoch 3...\n",
      "Generating negative pairs for epoch 3...\n",
      "Epoch 3: Generated 16992 pairs (8496 pos, 8496 neg)\n",
      "Training dataset: 16992 pairs\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21882s\u001b[0m 41s/step\n",
      "Loss: 0.6226, Train Acc: 0.6510, Val Acc: 0.6706\n",
      "üéØ New best validation accuracy: 0.6706\n",
      "\n",
      "=== Epoch 5/20 ===\n",
      "Generating positive pairs for epoch 4...\n",
      "Generating negative pairs for epoch 4...\n",
      "Epoch 4: Generated 17018 pairs (8509 pos, 8509 neg)\n",
      "Training dataset: 17018 pairs\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27878s\u001b[0m 53s/step\n",
      "Loss: 0.5782, Train Acc: 0.7072, Val Acc: 0.7096\n",
      "üéØ New best validation accuracy: 0.7096\n",
      "\n",
      "=== Epoch 6/20 ===\n",
      "Generating positive pairs for epoch 5...\n",
      "Generating negative pairs for epoch 5...\n",
      "Epoch 5: Generated 17036 pairs (8518 pos, 8518 neg)\n",
      "Training dataset: 17036 pairs\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19018s\u001b[0m 36s/step\n",
      "Loss: 0.5437, Train Acc: 0.7344, Val Acc: 0.7077\n",
      "\n",
      "=== Epoch 7/20 ===\n",
      "Generating positive pairs for epoch 6...\n",
      "Generating negative pairs for epoch 6...\n",
      "Epoch 6: Generated 17044 pairs (8522 pos, 8522 neg)\n",
      "Training dataset: 17044 pairs\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20013s\u001b[0m 38s/step\n",
      "Loss: 0.5122, Train Acc: 0.7574, Val Acc: 0.7383\n",
      "üéØ New best validation accuracy: 0.7383\n",
      "\n",
      "=== Epoch 8/20 ===\n",
      "Generating positive pairs for epoch 7...\n",
      "Generating negative pairs for epoch 7...\n",
      "Epoch 7: Generated 17046 pairs (8523 pos, 8523 neg)\n",
      "Training dataset: 17046 pairs\n",
      "\u001b[1m509/532\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m14:11\u001b[0m 37s/step"
     ]
    }
   ],
   "source": [
    "trained_model = train_with_dynamic_datasets(\n",
    "    model=siamese_model,\n",
    "    train_directory=TRAIN,\n",
    "    test_directory=TEST,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad790596",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save('face_verification_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29346b68",
   "metadata": {},
   "source": [
    "### Test / Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "y_hat = siamese_model.predict([test_in, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e706660",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Recall()\n",
    "m.update_state(y_true, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Precision()\n",
    "m.update_state(y_true, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_in[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('face_verification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('face_verification.h5', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ce63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_two_images(img1_path, img2_path, model):\n",
    "    def preprocess_single_image(img_path):\n",
    "        byte_img = tf.io.read_file(img_path)\n",
    "        img = tf.io.decode_jpeg(byte_img)\n",
    "        img = tf.image.resize(img, (100, 100))\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "\n",
    "    img1 = preprocess_single_image(img1_path)\n",
    "    img2 = preprocess_single_image(img2_path)\n",
    "\n",
    "    # Add batch dimension\n",
    "    img1 = tf.expand_dims(img1, axis=0)\n",
    "    img2 = tf.expand_dims(img2, axis=0)\n",
    "\n",
    "    # Predict similarity\n",
    "    result = model.predict([img1, img2])\n",
    "    print(f\"Similarity score: {result[0][0]:.4f}\")\n",
    "    \n",
    "    if result[0][0] > 0.5:\n",
    "        print(\"‚úÖ Match: Likely the same person\")\n",
    "    else:\n",
    "        print(\"‚ùå No Match: Likely different people\")\n",
    "\n",
    "test_on_two_images('C:/Users/lokna/Projects/MyReactNativeApp/extensions/face_auth/data/positive/496e69d0-3499-11f0-a4ad-e8fb1c79b654.jpg', 'C:/Users/lokna/Projects/MyReactNativeApp/extensions/face_auth/data/positive/4a5df055-3499-11f0-ae89-e8fb1c79b654.jpg', model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
